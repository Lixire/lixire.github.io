<!--
1: Using prettify
-->

<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>STAT 230</title>

<link rel="stylesheet" type="text/css" href="https://www.dropbox.com/s/atz4kqwayz4hlhk/markDown.css?raw=1">

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?skin=desert"></script>

</head>
<body><h1 id="stat-230-probability">STAT 230 - Probability</h1>

<p><div class="toc">
<ul>
<li><a href="#stat-230-probability">STAT 230 - Probability</a><ul>
<li><a href="#lecture-1">Lecture - 1</a><ul>
<li><a href="#the-3-definitions-of-probability">The 3 Definitions of Probability</a></li>
<li><a href="#how-does-prob-relate-to-cs">How does prob relate to CS?</a></li>
</ul>
</li>
<li><a href="#lecture-2-sept-12">Lecture 2 - Sept 12</a><ul>
<li><a href="#definitions">Definitions</a></li>
</ul>
</li>
<li><a href="#lecture-3">Lecture 3</a><ul>
<li><a href="#counting-techniques">Counting Techniques</a></li>
<li><a href="#sampling">Sampling</a></li>
<li><a href="#permutations">Permutations</a></li>
</ul>
</li>
<li><a href="#lecture-4">Lecture - 4</a><ul>
<li><a href="#combinations">Combinations</a></li>
<li><a href="#pascals-identity">Pascal’s Identity</a></li>
</ul>
</li>
<li><a href="#lecture-5">Lecture - 5</a><ul>
<li><a href="#arrangements-where-some-objects-are-alike-indistinguishable">Arrangements, where some objects are alike (indistinguishable)</a></li>
</ul>
</li>
<li><a href="#lecture-6">Lecture - 6</a><ul>
<li><a href="#de-morgans-laws">De Morgan’s Laws</a></li>
</ul>
</li>
<li><a href="#lecture-7">Lecture - 7</a><ul>
<li><a href="#multiplication-rule">Multiplication Rule</a></li>
</ul>
</li>
<li><a href="#lecture-8">Lecture - 8</a><ul>
<li><a href="#product-rule-45">Product Rule (4.5)</a></li>
<li><a href="#law-of-total-probability">Law of Total Probability</a></li>
</ul>
</li>
<li><a href="#lecture-9">Lecture - 9</a><ul>
<li><a href="#bayes-rule">Bayes’ Rule</a></li>
<li><a href="#machine-learning">Machine Learning</a></li>
<li><a href="#monty-hall-problem">“Monty Hall” problem</a></li>
<li><a href="#important-sums">Important Sums</a></li>
</ul>
</li>
<li><a href="#lecture-10">Lecture - 10</a><ul>
<li><a href="#ch-5-random-variables">Ch 5 Random Variables</a></li>
</ul>
</li>
<li><a href="#lecture-11">Lecture - 11</a><ul>
<li><a href="#properties-of-fx">Properties of F(x)</a></li>
<li><a href="#discrete-uniform-52">Discrete Uniform (5.2)</a></li>
</ul>
</li>
<li><a href="#lecture-12">Lecture 12</a><ul>
<li><a href="#hypergeometric-rv-53">Hypergeometric rv (5.3)</a></li>
<li><a href="#binomial-rv-54">Binomial rv (5.4)</a></li>
<li><a href="#tut-1">Tut 1</a></li>
</ul>
</li>
<li><a href="#lecture-13">Lecture 13</a><ul>
<li><a href="#bin-approx-to-hyp">Bin approx to Hyp.</a></li>
</ul>
</li>
<li><a href="#lecture-14">Lecture 14</a><ul>
<li><a href="#geometric-rv-56">Geometric rv (5.6)</a></li>
</ul>
</li>
<li><a href="#lecture-15">Lecture - 15</a><ul>
<li><a href="#poisson-rv-57">Poisson rv (5.7)</a></li>
<li><a href="#poisson-process-58">Poisson Process (5.8)</a></li>
</ul>
</li>
<li><a href="#lecture-16">Lecture - 16</a><ul>
<li><a href="#combining-models-59">Combining Models (5.9)</a></li>
</ul>
</li>
<li><a href="#lecture-17">Lecture 17</a><ul>
<li><a href="#ch-7-expected-value-variance-summarizing-data">Ch 7. Expected Value + Variance Summarizing Data</a></li>
</ul>
</li>
<li><a href="#lecture-18">Lecture 18</a><ul>
<li><a href="#applications-of-expectation-53">Applications of Expectation (5.3)</a></li>
</ul>
</li>
<li><a href="#lecture-19">Lecture 19</a><ul>
<li><a href="#means-and-variances-of-named-distributions-74">Means (and variances) of named distributions (7.4)</a></li>
</ul>
</li>
<li><a href="#lecture-20">Lecture 20</a><ul>
<li><a href="#mean-var-of-a-linear-fn-of-x">Mean + Var of a linear f’n of X</a></li>
<li><a href="#variances-of-named-distributions">Variances of named distributions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</p>



<h2 id="lecture-1">Lecture - 1</h2>

<blockquote>
  <p>Lecturer: Diana Skrzydlo (Skryzlo) <br>
   Office Hours: MF 3:30-5 in M3 3106 &amp; W 11:30-12:30 in M3 3144</p>
</blockquote>

<p>What is probability? <br>
The likelihood of an event occuring</p>



<h3 id="the-3-definitions-of-probability">The 3 Definitions of Probability</h3>

<ol>
<li>Classical - Probability is equal to <script type="math/tex" id="MathJax-Element-1">\frac{\text{# of ways event occurs}}{\text{total # of outcomes}}</script> (relies on outcomes being equally likely)</li>
<li>Relative Frequency - Probability = proportion of time (<script type="math/tex" id="MathJax-Element-2">\infty</script> long) that the event occurs</li>
<li>Subjective - Probability = how certain the person is that it will occur</li>
</ol>



<h3 id="how-does-prob-relate-to-cs">How does prob relate to CS?</h3>

<ul>
<li>optimizing runtime</li>
<li>randomized algos</li>
<li>confidence bounds on answers</li>
<li>recommendation engine</li>
<li>targeted ads</li>
<li>machine learning</li>
</ul>

<p>Definitions to set up a probability model</p>



<h2 id="lecture-2-sept-12">Lecture 2 - Sept 12</h2>

<ul>
<li>Definitions for probability models: experiment, trial, outcome, sample space</li>
<li>events</li>
<li>probability</li>
<li>example: binary search trees</li>
</ul>

<p>Review Question:</p>

<blockquote>
  <p>What is essential to the classical definition of probability? <br>
  Outcomes are equally likely</p>
</blockquote>



<h3 id="definitions">Definitions</h3>

<dl>
<dt>Experiment</dt>
<dd>A process that can be repeated multiple times, with multiple results</dd>

<dt>Trial</dt>
<dd>One iteration of an experiment</dd>

<dt>Outcome</dt>
<dd>The result on one trial of an experiment</dd>

<dt>Sample Space</dt>
<dd>A set of all possible outcomes for a single trial of an experiment. Notes: S is a set. i.e. an unordered, unique element list. S must contain ALL possible outcomes. Two types: discrete (finite) -&gt; can list/count, continuous -&gt; uncountably many elements

<blockquote>
  Example <br>
  Experiment: Roll a 6-sided die repeatedly <br>
  Trial: One roll <br>
  Outcomes: 1, 2, … , 5, 6 <br>
  <script type="math/tex" id="MathJax-Element-3">S_1</script> = {1, 2, 3, 4, 5, 6} (most versatile) <br>
  <script type="math/tex" id="MathJax-Element-4">S_2</script> = {even, odd} (also works)
</blockquote></dd>

<dt>Event</dt>
<dd>A subset of a sample space (including the empty set <script type="math/tex" id="MathJax-Element-5">\phi</script>) and the entire sample space <script type="math/tex" id="MathJax-Element-6">S</script>

<blockquote>
  Example <br>
  A = “a 1 is rolled” -&gt; A = {1} <br>
  B = “the number is odd” -&gt; B = {1, 3, 5} <br>
  <strong>Note:</strong> events consisting of one sample point are simple events, otherwise they are called compound events
  
  On any trial, if the outcome is in the event A, we say A has occurred. <br>
  <strong>E.g.</strong> if a 1 is rolled -&gt; A occurs, B occurs <br>
  3 is rolled -&gt; B occurs, A does not <br>
  6 is rolled -&gt; neither occur
</blockquote></dd>

<dt>A Probability</dt>
<dd>
<p>is a function that maps events in a sample space to <script type="math/tex" id="MathJax-Element-7">Real</script> numbers such that </p>

<ol>
<li><script type="math/tex" id="MathJax-Element-8">0 \leq P(A) \leq 1</script> for any A <br>
<ul><li><script type="math/tex" id="MathJax-Element-9">0 =</script> “impossible”</li>
<li><script type="math/tex" id="MathJax-Element-10">1 =</script> “guaranteed”</li></ul></li>
<li>If <script type="math/tex" id="MathJax-Element-11">a_i</script> are the elements of S, then <script type="math/tex" id="MathJax-Element-12">\Sigma_{\text{all ai}} P(a_i) = 1</script> <br>
<ul><li>where Simple event <script type="math/tex" id="MathJax-Element-13">A_i = \{a_i\}</script> </li>
<li>Example: Roll a 6-sided die twice. S = {(1,1), (1,2), … , (1,6), … , (6, 1), … , (6,6)}</li>
<li>Example: BST. A Binary Search Tree is degenerate if each node has at most one child</li>
<li>Find the prob. that a tree with 3 elements is degenerate: </li>
<li>We drew out all probs. 4/6 chance</li></ul></li>
</ol>
</dd>
</dl>



<h2 id="lecture-3">Lecture 3</h2>

<p>Recall</p>

<blockquote>
  <p>P(A) is the probability that A occurs. A is a subset of S. If A is compound, its prob is the sum of the probs of the simple events that make up A.</p>
</blockquote>



<h3 id="counting-techniques">Counting Techniques</h3>

<p>Two basic counting rules:</p>

<ol>
<li>Addition rule <br>
<ul><li>if you can do 1 task in p ways and task 2 in q ways. Then the number of ways to do task 1 XOR task 2 is just p + q.</li></ul></li>
<li>Multiplication rule <br>
<ul><li>If we can do task 1 in p ways and for each of those ways, we can do task 2 in q ways, then the total number of ways we can do task 1 AND 2 is pq.</li></ul></li>
</ol>



<h3 id="sampling">Sampling</h3>

<p>“with replacement” - is possible to obtain the same result more than once. Then it’ll have <script type="math/tex" id="MathJax-Element-14">m^k</script> choices. M spaces, k options.</p>



<h3 id="permutations">Permutations</h3>

<p>A permutation is an ordering of k objects selected from n objects without replacement. Let n = number of objects. K spaces. The order matters! <script type="math/tex" id="MathJax-Element-15">n^{(k)}</script> = “n to k factors” = <script type="math/tex" id="MathJax-Element-16">n(n-1)...(n-k+1)</script> <br>
Also <script type="math/tex" id="MathJax-Element-17">\frac{n!}{(n-k)!}</script></p>

<blockquote>
  <p>Tip, if computing a certain permutation is hard, try 1 - opposite of what you want to find. Eg. Find P(of at least one repeat) turns into 1 - P(no repeats) which is easier.</p>
</blockquote>



<h2 id="lecture-4">Lecture - 4</h2>



<h3 id="combinations">Combinations</h3>

<p>A combination is a selection of k objects from n objects without replacement. The order does NOT matter. E.g. drawing a hand of cards. Denoted by N choose K (I forget how to <script type="math/tex" id="MathJax-Element-18">\LaTeX</script> this) Pascal triangle - n choose k is the kth entry in the nth row.</p>

<p>Some shit about pascal’s triangle that is super obvious</p>



<h3 id="pascals-identity">Pascal’s Identity</h3>

<blockquote>
  <p>(n choose k) = (n-1 choose k-1) + (n-1 choose k) <br>
  We won’t prove this mathematically since it’s messy to prove</p>
</blockquote>

<p>oh lool, we proved this in MATH 239.</p>



<h2 id="lecture-5">Lecture - 5</h2>

<p>Example: <br>
A box of 10 computer chips containing 3 defective ones in the box. Four of the chips are tested. What is the probability that 1 is found defective? <br>
Solution: <br>
W/o rep &amp; don’t care about order.</p>

<p><script type="math/tex" id="MathJax-Element-19">Prob = \frac{\text{# of ways to get 1 def chips}}{\text{# of ways to test 4}} = \frac{\binom{3}{1}\binom{7}{3}}{\binom{10}{4}}</script></p>

<p>The whole time, we don’t care about order. (i.e. we don’t need to worry if the defective one is first, last or etc)</p>

<blockquote>
  <p>Note: “1 if found” means exactly 1. Not at least one.</p>
</blockquote>



<h3 id="arrangements-where-some-objects-are-alike-indistinguishable">Arrangements, where some objects are alike (indistinguishable)</h3>

<blockquote>
  <p>Can be arranged  <script type="math/tex" id="MathJax-Element-20">\frac{n!}{n_1!n_2!...n_k!}</script> where <script type="math/tex" id="MathJax-Element-21">n_1 + n_2 + ... + n_k = n</script> and <script type="math/tex" id="MathJax-Element-22">n_k</script> is number of type k.</p>
</blockquote>



<h2 id="lecture-6">Lecture - 6</h2>

<p>Recall - event -&gt; subset of S for any event A, <script type="math/tex" id="MathJax-Element-23">0 \leq P(A) \leq 1</script> <br>
Rules for probabilities <br>
If an event A is contained in another event B (A subset B) Then, <script type="math/tex" id="MathJax-Element-24">P(A) \leq P(B)</script></p>

<p><script type="math/tex" id="MathJax-Element-25">P(B) = \Sigma_{a_i \epsilon B} P(a_i)</script> <br>
<script type="math/tex" id="MathJax-Element-26">= \Sigma_{a_i \epsilon A} P(a_i) + \Sigma_{a_i \epsilon B but not A}P(a_i)</script> <br>
<script type="math/tex" id="MathJax-Element-27">= P(A) + (\geq 0)</script> <br>
thus, <script type="math/tex" id="MathJax-Element-28">P(A) \leq P(B)</script></p>

<p>A_bar , A^c, A^’, -&gt; NOT A. Aunion B = or. A intersect B = and.</p>



<h3 id="de-morgans-laws">De Morgan’s Laws</h3>

<p>Bar{aunionB} = a_bar and b_bar <br>
bar{aintersectB} = a_bar or b_bar</p>

<p>P(a union B) = P(A) + P(B) - P(AB)</p>



<h2 id="lecture-7">Lecture - 7</h2>



<h3 id="multiplication-rule">Multiplication Rule</h3>

<p>applies when events are independent of each other (don’t affect one another)</p>



<h2 id="lecture-8">Lecture - 8</h2>

<p>Recall: Independent: P(AB) = P(A)P(B) <br>
Conditional prob of A given B: P(A|B) = <script type="math/tex" id="MathJax-Element-29">\frac{P(AB)}{P(B)}</script></p>

<p>Last time we had <br>
A = small = 3 <br>
C = total = 8 <br>
P(A|C) = 1/5 <br>
P(A) = 1/6</p>

<p>C occuring makes A more likely.</p>

<p>P(C|A) = 1/6, P(C) = 5/36</p>

<p>A occurring makes C more likely. Dependence is a two-way relationship</p>

<p>Alternative defn of independence. P(A|B) = P(A) <br>
knowing B occurred has no effect on the probability of A occurring. P(B|A) = P(B)</p>

<p>Example:</p>

<p>Diana has 2 kids. Each independently has 0.5 chance of having red hair. At least one has red hair. What is the probability the other does? Let A = at least one red</p>

<table>
<thead>
<tr>
  <th></th>
  <th>Kid 1</th>
  <th>Kid 2</th>
</tr>
</thead>
<tbody><tr>
  <td><code>red</code></td>
  <td>0.25</td>
  <td>0.25</td>
</tr>
<tr>
  <td><code>not red</code></td>
  <td>0.25</td>
  <td>0.25</td>
</tr>
</tbody></table>


<p>P(A) = 0.75</p>

<p>P(both|A) = P(both and A) / P(A) = 0.25/0.75 = 1/3</p>



<h3 id="product-rule-45">Product Rule (4.5)</h3>

<p>If we rearrange the definition of conditional prob, we get: <br>
P(AB) = P(B)P(A|B) <br>
P(AB) = P(A)P(B|A)</p>

<p>Intuitively, think of “checking” whether A occurred. If it did, “check” whether B occurred (knowing A did). (or you check B first, then A) Works for both dependent and independent events.</p>

<p>Extends nicely to multiple events P(ABC) = P(B)P(C|B)P(A|BC) <br>
The order doesn’t matter as long as each probability is conditional on what we know.</p>



<h3 id="law-of-total-probability">Law of Total Probability</h3>

<p>Let <script type="math/tex" id="MathJax-Element-30">A_1 , ... , A_k</script> be a collection of events that are mutually exclusive <script type="math/tex" id="MathJax-Element-31">A_i\cap A_j = \phi</script> cover the sample space <script type="math/tex" id="MathJax-Element-32">A_1 \cup ... \cup A_k = S</script></p>

<p>(break S up into k disjoint pieces) This is a partition of S <br>
e.g. A and A_bar is a partition of size 2. <br>
For any event B in S, we can express it as B = <script type="math/tex" id="MathJax-Element-33">\cup_{i=1}^{k} (BA_i)</script> <br>
So P(B) = <script type="math/tex" id="MathJax-Element-34">\Sigma_{i=1}^{k}P(A_i)P(B|A_i)</script> using product rule</p>

<p>Example: <br>
Three people A,B,C are writing code together. A produces twice as much as B or C</p>

<p>1% of A’s code has errors <br>
2% of B’s <br>
5% of C’s <br>
What is the probability that a random line of code has errors?</p>

<p>Let E = line has error <br>
A = line written by A <br>
B = line written by B <br>
C = line written by C</p>

<p>We want P(E)</p>

<p>Using the partition {A, B, C}, P(A) = 0.5 , P(B) = P(C)  = 0.25 <br>
We have P(E|A) = 0.01 <br>
P(E|B) = 0.02 <br>
P(E|C) = 0.05</p>

<p>So <script type="math/tex" id="MathJax-Element-35">P(E) = P(A)P(E|A) + P(B)P(E|B) + P(C)P(E|C) = 0.5*0.01 + 0.25*0.02 + 0.25*0.05 = 0.0225</script></p>



<h2 id="lecture-9">Lecture - 9</h2>

<p>We found P(E) = 0.0225 using law of total prob. We would like to know, if we find an error, whose fault is it? <br>
We had to reverse the direction of the conditioning: we have P(E|author). We want P(author|E).</p>



<h3 id="bayes-rule">Bayes’ Rule</h3>

<p><script type="math/tex" id="MathJax-Element-36">P(A|B) = \frac{P(AB)}{P(B)} = \frac{P(A)P(B|A)}{P(B)}</script></p>

<p>also written as <br>
<script type="math/tex" id="MathJax-Element-37">P(A|B) = \frac{P(A)P(B|A)}{\Sigma_{i=1}^k P(A_i)P(B|A_i)}</script></p>

<p>For this example: <br>
<script type="math/tex" id="MathJax-Element-38">P(A|E) = \frac{0.5*0.01}{0.0225} = 0.222...</script></p>

<p>Example</p>

<blockquote>
  <p>Your spam filter has 90% detection rate but 1% false positive rate. Suppose 25% of messages are spam</p>
</blockquote>

<p>Let S = “message is spam” P(S) = 0.25 P(S’) = 0.75 <br>
I = “Identified as spam”  <br>
P(I/S) = 0.9 -&gt; P(I’|S) = 0.0</p>

<p>photo here&lt;&gt;</p>



<h3 id="machine-learning">Machine Learning</h3>

<p>Supervised learning -&gt; give some training data <br>
Unsupervised learning -&gt; no training data</p>

<dl>
<dt>Types of Problems</dt>
<dt>Classification</dt>
<dd>spam vs non-spam email</dd>

<dd>cancerous vs benign tumor</dd>

<dd>fraudulent vs legit banking transactions</dd>

<dt>Regression</dt>
<dd>predict the price of house/stock</dd>

<dd>length of recovery time</dd>

<dt>Clustering</dt>
<dd>grouping products together</dd>

<dd>making recommendations</dd>
</dl>

<p>Bayesian Classifier <br>
P(Category 1 | evidence) =<script type="math/tex" id="MathJax-Element-39">\frac{P(cat1)P(ev|cat1)}{P(evidence)}</script></p>



<h3 id="monty-hall-problem">“Monty Hall” problem</h3>

<p>3 doors. 1 door is revealed to be a dud. Do you switch doors?</p>

<p>Yes</p>



<h3 id="important-sums">Important Sums</h3>

<ol>
<li>Geometric - <script type="math/tex" id="MathJax-Element-40">\frac{a}{1-r}</script></li>
<li>Binomial - <script type="math/tex" id="MathJax-Element-41">(a+b)^n = \Sigma</script></li>
</ol>



<h2 id="lecture-10">Lecture - 10</h2>

<p>Bayes’ Rule:</p>

<blockquote>
  <p>P(A|B) = <script type="math/tex" id="MathJax-Element-42">\frac{P(A)P(B|A)}{P(B)}</script> <br>
  Product rule + law of total prob. A and A’ is a partition</p>
</blockquote>



<h3 id="ch-5-random-variables">Ch 5 Random Variables</h3>

<blockquote>
  <p>Def. A random variable (rv) is a function that maps points in a sample space to Real numbers</p>
</blockquote>

<p>The values that the rv can take on are called the RANGE of the rv. By convention, we call rvs X,Y,Z and the values, x,y,z <br>
We are interested in finding the prob. that a rv X takes on one of it’s particular values. ie P(X =x) (outcome is a sample point that maps to x)</p>

<p>There are two types of rvs, discrete -&gt; finite or countably infinite <strong>range</strong> vs continuous -&gt; incountably infinite <strong>range</strong></p>

<p>More than one rv can be defined on the same S</p>

<blockquote>
  <p>eg roll 3 fair 6-sided dice. <br>
  X = sum on 3 dice {3, … , 18} <br>
  Y = avg value {1, … , 6} <br>
  Z = the 3 digit number created by the dice <br>
  W = the product of the 3 {1, 2, … , 216} <br>
  U = number of 3s {1,2,3}  <br>
  etc</p>
</blockquote>

<hr>

<blockquote>
  <p>Def. The probability factor (pf) of a discrete rv X is f(x) = P(X = x) and is only defined for x <script type="math/tex" id="MathJax-Element-43">\epsilon</script> range of X. f(x) if x does not belong in range is undefined or 0.</p>
  
  <dl>
<dt>Properties of f(x)</dt>
<dd><script type="math/tex" id="MathJax-Element-44">0 \leq f(x) \leq 1</script> for all x <script type="math/tex" id="MathJax-Element-45">\epsilon</script> range of f(x) . Why? it’s a probability</dd>

<dd><script type="math/tex" id="MathJax-Element-46">\Sigma_{\text{all x}\ \epsilon \ \text{range}} f(x) = 1</script> The events “X = x” are mutually exclusive for each x</dd>
</dl>
</blockquote>

<hr>

<blockquote>
  <p>Def. The cumulative distribution function (cdf) of the rv X is F(x) = P(X <script type="math/tex" id="MathJax-Element-47">\leq</script> x) and is defined for ALL Real x.</p>
</blockquote>



<h2 id="lecture-11">Lecture - 11</h2>

<blockquote>
  <p>Recall: Random variable (X) maps sample points to Real numbers <br>
  Probability function f(x) = P(X=x) <script type="math/tex" id="MathJax-Element-48">\forall</script> x <script type="math/tex" id="MathJax-Element-49">\epsilon</script> range <br>
  Cumulative distribution function F(x) = P(<script type="math/tex" id="MathJax-Element-50">X \leq x</script>) <script type="math/tex" id="MathJax-Element-51">\forall</script> x <script type="math/tex" id="MathJax-Element-52">\epsilon R</script> </p>
</blockquote>



<h3 id="properties-of-fx">Properties of F(x)</h3>

<ol>
<li><script type="math/tex" id="MathJax-Element-53">0 \leq F(x) \leq 1</script> Why? it’s a probability</li>
<li>F(x) is non-decreasing with respect to x . Why? Can’t lose probability when increasing x <br>
<ol><li>Proof: the event “<script type="math/tex" id="MathJax-Element-54">X \leq a</script>” is contained in “<script type="math/tex" id="MathJax-Element-55">X \leq b</script>” where <script type="math/tex" id="MathJax-Element-56">a \leq b</script> so <script type="math/tex" id="MathJax-Element-57">F(a) \leq F(b)</script></li></ol></li>
<li><script type="math/tex" id="MathJax-Element-58">lim_{x \rightarrow - \infty} F(x) = 0</script> where 0 is impossible</li>
<li><script type="math/tex" id="MathJax-Element-59">lim_{x \rightarrow \infty} F(x) = 1</script> where 1 is guarenteed</li>
</ol>

<p>Example:</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>0</th>
  <th>1</th>
  <th>2</th>
  <th>3</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>125/216</td>
  <td>75/216</td>
  <td>15/216</td>
  <td>1/216</td>
</tr>
<tr>
  <td>F(x)</td>
  <td>125/216</td>
  <td>200/216</td>
  <td>215/216</td>
  <td>1</td>
</tr>
</tbody></table>


<p>Let’s graph it to show what happens at other Real values of x. <br>
<img src="http://i.stack.imgur.com/nzR9r.png"></p>

<ul>
<li>needs to approach one</li>
<li>needs to approach zero</li>
<li>right-step function (holes are filled on the left. empty on the right)</li>
<li>On this image, needs to have an arrow from 0 going to the left</li>
</ul>

<p>Relationship between F and f <br>
f(x) = the size of the jump in F(x) at the point x = F(x) - F(x-1) &lt;- which is the next smallest value in range</p>

<p><script type="math/tex" id="MathJax-Element-60">F(x) = f(0) + f(1) + ... + f(\lfloor x \rfloor) = \Sigma_{i=0}^{\lfloor x \rfloor} f(i)</script> <br>
We can use F(x) to find <script type="math/tex" id="MathJax-Element-61">P(a < X \leq b)</script> = P()</p>



<h3 id="discrete-uniform-52">Discrete Uniform (5.2)</h3>

<p>Let X be a rv on the range {a, a+1, …, b} where <script type="math/tex" id="MathJax-Element-62">a \leq b</script> &amp; a,b <script type="math/tex" id="MathJax-Element-63">\epsilon Z</script> where each value is equally likely. Then we say X has a discrete uniform distribution on [a , b] in other words, “X~DU[a,b]” . <em>X has a DU distribution of [a,b]</em></p>

<p>e.g. </p>

<ul>
<li>fair die ~DU[1,6] </li>
<li>position in a deck of one particular card ~ DU [1, 52]</li>
</ul>

<p>Find the pf f(x) = P(X = x) = c must be constant since all equal <br>
We need <script type="math/tex" id="MathJax-Element-64">\Sigma_{x = a}^{b}f(x) = 1</script> <br>
<script type="math/tex" id="MathJax-Element-65">\Sigma_{x=a}^{b}c  = 1 </script> <br>
<script type="math/tex" id="MathJax-Element-66">(b-a+1)c  = 1 </script> <br>
<script type="math/tex" id="MathJax-Element-67">C = \frac{1}{b-a+1} = f(x) </script></p>

<p>Find the cdf <br>
<script type="math/tex; mode=display" id="MathJax-Element-68">\begin{equation}
\begin{split}
F(x) & = \Sigma_{i = a}^{\lfloor x \rfloor}f(i) \\ 
& = \Sigma_{a}^{\lfloor x\rfloor} \frac{1}{b-a+1} \\
F(x) & = 
 \left\{
\begin{array}{ll}
      \frac{\lfloor x \rfloor -a + 1}{b-a+1} & a\leq x \leq b \\
      0 & x < a\\
      1 & x > b \\
\end{array} 
\right.
\end{split}
\end{equation}</script></p>

<p>Example</p>

<blockquote>
  <p>You are looking through a linked list of 100 items for one particular item. Let X = # of comparisons to find it. Claim: X ~ DU [1, 100]</p>
</blockquote>



<p><script type="math/tex; mode=display" id="MathJax-Element-69">\begin{equation}
\begin{split}
f(1) &= P(X=1) = \frac{1}{100} \\
f(2) & = P(X =2) \\
& = P(X \neq 1)P(X=2|X\neq 1) \\
& = \frac{99}{100}*\frac{1}{99} = \frac{1}{100} \\
similarly, all f(x)  &= \frac{1}{100}
\end{split}
\end{equation}</script></p>



<h2 id="lecture-12">Lecture 12</h2>



<h3 id="hypergeometric-rv-53">Hypergeometric rv (5.3)</h3>

<p>Set up: we have N objects -&gt; r “successes”, N-r “Failures” <br>
We choose n without replacement</p>

<p>Let X = “# of S’s chosen” <br>
eg. if x=# of winning numbers on a lotto 6/49 ticket <br>
X ~ Hyp(N,r,n) <br>
N=49, r = 6, n =6 <br>
Find the pf f(x) = P(X=x) = P(we get x S’s and n-x F’s)</p>

<p><script type="math/tex" id="MathJax-Element-70">f(x) = \frac{{r \choose x}{N-r \choose n-x}}{N \choose n}</script> <br>
range of x? <br>
lower bound: <script type="math/tex" id="MathJax-Element-71">x \leq 0</script>, <script type="math/tex" id="MathJax-Element-72">x \leq n - (N-r)</script> possible to run out of F’s so remaining will all be S’s</p>

<p>upper bound x \leq n (all 5’s), x \leq r (could run out of S’s) <br>
so the range of X is messy and depends on the relationship btwn N, r, and n.</p>

<p>Also, there is no closed form expression for F(x). You have to add up the f(x) values which is computationally intensive when N is large. <br>
Example: <br>
You have 10 cards - 7 treasure, 3 non-treasure. <br>
Draw 5 without replacement. Let X = “# of treasure cards”. X ~ Hyp(N=10, r=7, n=5) <br>
f(x) = P(X = x) = <script type="math/tex" id="MathJax-Element-73">\frac{{7 \choose x}{3 \choose 5-x}}{10 \choose 5}</script> for x = 2,3,4,5 <br>
All the values of f(x) will add up to 1 <br>
(proven using Hypergeometric series result)</p>



<h3 id="binomial-rv-54">Binomial rv (5.4)</h3>

<p>Set up: Bernoulli trials - independent trials, 2 outcomes on each (S or F), P(S) = P is constant for all trials</p>

<p>We do n trials. <br>
Let X = # of S in n trials <br>
(You can imagine it as a Hypergeometric selecting with replacement instead of without.) <br>
We write X ~ Bin(n, p) <br>
Eg. Flip a fair coin 10 times, x = # heads, X ~ Bin (n = 10, p = 0.5) <br>
Find the pf f(x) = P(X=x) = P(we get x S’s and n-x F’s) = p^x(1-p)^(n-x)j</p>



<h3 id="tut-1">Tut 1</h3>

<ol>
<li>The letters of PROBABILITY are arranged at random in a row. Find the probability that: <br>
<ol><li>Y is in the last position <br>
<ul><li>Total # is 9979200</li>
<li># to have Y last = 907200</li>
<li>so Prob = 1/11 (also can just consider that 1/11 chance last letter is y)</li></ul></li>
<li>The two B’s are consecutive <br>
<ul><li>treat “BB as one letter</li>
<li>P = (10!/2!)/(11!/(2!2!)) = 0.182</li></ul></li>
<li>The two B’s are consecutive and Y is in the last position <br>
<ul><li>(9!/2!)/(11!/(2!2!)) = 0.018 = P(AB)</li>
<li>Note: <script type="math/tex" id="MathJax-Element-74">P(AB) \neq P(A)P(B)</script></li></ul></li>
<li>Y is not in the last position and the two B’s are not consecutive <br>
<ul><li>P(A_bar B_bar) = P((AUB)_bar) = 1-P(AUB) = 1-(P(A)+P(B)-P(AB)) = 0.745</li></ul></li>
<li>Y is not in the last position or the two B’s are not consecutive <br>
<ul><li>P((AB)_bar) = P(A_bar U B_bar) = 1-P(AB) = 0.982</li></ul></li></ol></li>
<li>You are trying to guess your friend’s Quest password. You know it must be 8 characters chosen from digits 0-9, lower case letters a-z, and uppser case letters A-Z and is not allowed to be all letters or all numbers <br>
<ol><li>how many possible valid passwords are there? <br>
<ul><li>62 possible characters</li>
<li>62^8 - 52^8 - 10^8</li></ul></li>
<li>You happen to know that your friend is a bit lazy with respect to password security and will only use the letters a,s,d and f (both upper and lower) and the number 1. How many possible valid passwords could your friend have? <br>
<ul><li>now 1 + 4 + 4 = 9 characters</li>
<li>So 9^8  - 8^8 - 1 = 26269504</li></ul></li>
<li>What is the probability your friend’s password has no repeated characters? <br>
<ul><li>no repeats - selecting w/o replacement so order still matters </li>
<li>So 9^8 - 8^8 - 0 = 322560</li>
<li>P = 322560/26269504 = 0.012</li></ul></li>
<li>What is the probability your friend’s password contains at least one “a” or “A”? <br>
<ul><li>1 - P(no “a” or “A”s)</li>
<li># w/o = 7^8 -6^8 -1^8</li>
<li>P = 1 - (7^8 -6^8 -1^8)/( 9^8  - 8^8 - 1) = 0.844</li></ul></li></ol></li>
<li>Consider the machine learning problem of classifying incoming messages as spam. We define: A_1 = message fails rdns check (i.e. the “from” domain does match), A_2 = message is sent to over 100 people, A_3 = message contains a link with the url not matching the alt text. We will assume that the A_i’s are independent events, <strong>given</strong> that a message is spam, and that they are independent events, <strong>given</strong> that a message is regular. This is known as the “Naive Bayes Classifier” and is the simplest of the machine learning classification algorithms. We estimate P(A_1|Spam) = 0.3 ,P(A_2|Spam) = 0.2,P(A_3|Spam) = 0.1, P(A_1|Not Spam) = 0.005, P(A_2|Not Spam) = 0.04, P(A_3|Not Spam) = 0.05 and P(Spam) = 0.25 <br>
<ol><li>Suppose a message has all of features 1,2,and 3 present. Det P(Spam|A_1,A_2,A_3) <br>
<ul><li>= (P(Spam)P(A_1A_2A_3|Spam))/(P(Spam)P(A_1A_2A_3|Spam) +P(Spam_bar)P(A_1A_2A_3|Spam)) = (P(S)P(A_1|S)P(A_2|S)P(A_3|S))/(P(S)P(A_1|S)P(A_2|S)P(A_3|S) + P(S_bar)P(A_1|S)P(A_2|S)P(A_3|S)) = 0.995</li></ul></li>
<li>Suppose a message has features 1 and 2 present, but feature 3 is not present. Determine P(Spam | A_1A_2(A_3)bar). <br>
*(P(S)P(A_1|S)P(A_2|S)P(A_3_bar|S))/(P(S)P(A_1|S)P(A_2|S)P(A_3_bar|S) + P(S_bar)P(A_1|S)P(A_2|S)P(A_3_bar|S)) = 0.9895</li>
<li>If you declared as spam any message with one or more of features 1,2, or 3 present, what fraction of spam emails would you detect? <br>
<ul><li>P(A_1UA_2UA_3|Spam) = 1 - P(none of features) = 1-P(A_1_barA_2_barA_3_bar|spam) = 1- P(A_1_bar|Spam) … P(A_3_bar|Spam) = 0.496</li></ul></li>
<li>Given that a message is declared as spam (according to the rule in (c)), what is the probability that it actually is spam? <br>
<ul><li>similar to e. ans = 0.8508</li></ul></li></ol></li>
<li>Given that a message is declared as spam, (according to the rule in (c)), what is the probability that feature 1 is present? <br>
<ul><li>P(A_1|A_1UA_2UA_3) = P(A)/(P(A_1UA_2UA_3)) = P(S)P(A_1|S)+P(S_bar)P(A_2|S) )/(P(S)*0.496) = 0.641</li></ul></li>
<li>Let X represent the number of days in Feb. with temp below -24C. The probability function (pf) of X, f(x)= P(X=x) insert photo <br>
<ol><li>0.0625</li>
<li>look at one note</li>
<li>F(3.5) - F(0.5) = 0.375</li>
<li>f(2)/0.375 = 0.8333…</li></ol></li>
</ol>



<h2 id="lecture-13">Lecture 13</h2>

<blockquote>
  <p>Recall: <br>
  X~Hyp(N, r, n) <br>
  <script type="math/tex" id="MathJax-Element-75">f(x) = \frac{{r \choose x}{N-r \choose n-x}}{{N \choose n}}</script> <br>
  X = # S’s in n objects w/o rep <br>
  X~Bin(n,p) <br>
  <script type="math/tex" id="MathJax-Element-76">f(x) = {n \choose x}p^x (1-p)^{n-x}</script></p>
</blockquote>

<p>Example: <br>
Want to send a 4-bit message. Each bit is independently flipped (0-&gt;1 or 1-&gt;0) <br>
Probability = 0.1 <br>
P(message received correctly?) <br>
Let X = # of bits flipped <br>
X~Bin(4, 0.1) <br>
P(X=0) =<script type="math/tex" id="MathJax-Element-77">{4 \choose 0}0.1^00.9^4 = 0.656</script></p>

<p>Now add 3 “parity bits” to the message which allows the receiver to detect and fix up to 1 error. <br>
Let Y = # bits flipped ~Bin(7, 0.1) <br>
P(Y=0) + P(Y=1) are both ok. <br>
<script type="math/tex" id="MathJax-Element-78">= {7 \choose 0}0.1^00.9^7 + {7 \choose 1}0.1^10.9^6= 0.85</script></p>



<h3 id="bin-approx-to-hyp">Bin approx to Hyp.</h3>

<p>(ie. n &lt;&lt; N) then it doesn’t make a big difference to the probabilities if you sample with or without replacement. If we did it with replacement the number of S’s we get <br>
X~Bin(n,p=<script type="math/tex" id="MathJax-Element-79">\frac{r}{N}</script>) <br>
So when N is large and n is small, we can use a Bin(n, <script type="math/tex" id="MathJax-Element-80">\frac{r}{N}</script>) to approximate a Hyp(N,r,m). (when <script type="math/tex" id="MathJax-Element-81">\frac{n}{N}</script> is less than 0.05)</p>



<h2 id="lecture-14">Lecture 14</h2>

<blockquote>
  <p>recall: Negative Binomial (5.5) <br>
  Bernoulli trials (indep, S or F P(S)=p) X = # of F’s before the Kth S is obtained <br>
  pf and examples of NB <br>
  Geometric rv (5.6) <br>
  How to tell when to use distributions</p>
</blockquote>

<table>
<thead>
<tr>
  <th>Bin</th>
  <th>NB</th>
</tr>
</thead>
<tbody><tr>
  <td>-know # trials(n)</td>
  <td>-know # successes(k)</td>
</tr>
<tr>
  <td>-? successes modelled by X</td>
  <td>-? trails modelled by k+x</td>
</tr>
</tbody></table>


<p>We write <script type="math/tex" id="MathJax-Element-82">X\sim NB(k,p)</script> <br>
range <script type="math/tex" id="MathJax-Element-83">\{0,1,2, ...\}</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-84">\begin{equation}
\begin{split}
pf \ \ \ f(x) & = P(X=x) \\
& = P(x \ \ \text{F's befpre the kth S}) \\
& = {x+k-1 \choose k-1} p^k(1-p)^x \\
&{x+k-1 \choose k-1}  \ \text{is # orderings} \\
&p^k  \ \text{p of k S's} \\
&(1-p)^x  \ \text{p of x F's}
\end{split}
\end{equation}</script></p>

<p>We can show that <script type="math/tex" id="MathJax-Element-85">\Sigma^{\infty}_{x=0}f(x) = 1</script> <br>
But there is no closed form expression for <script type="math/tex" id="MathJax-Element-86">F(x)=\Sigma^{x}_{y=0}f(y)</script></p>

<blockquote>
  <p>Example: <br>
  A startup is looking for 5 investors. Each investor will independently say yes with probability 0.2. Founders will ask investors one at a time until they get 5 yes. Let X=total # of investors asked. Find f(x) and f(10). <br>
  <script type="math/tex" id="MathJax-Element-87">X \not\sim NB</script> <br>
  Let Y = # who say no (Y+5=X) <br>
  <script type="math/tex" id="MathJax-Element-88">Y \sim NB(5, 0.2)</script> <br>
  So <br>
  <script type="math/tex; mode=display" id="MathJax-Element-89">\begin{equation}
\begin{split}
f(x) & = P(X=x) \\ &= P(Y= x-5) \\ & ={x-5+5-1 \choose 5-1} {0.2}^5 0.8^{x-5} \\
& = {x-1 \choose 4}0.2^50.8^{x-5} \ \text{for x=5,6,7,...} \\
f(10) &= {9 \choose 4}0.2^50.8^5 =0.013
\end{split}
\end{equation}</script></p>
</blockquote>



<h3 id="geometric-rv-56">Geometric rv (5.6)</h3>

<p>(nothing to do with hypergeometric) <br>
Special case of NB with k=1. <br>
X= # of F’s before obtaining the first S in Bernoulli trails <br>
X~Geo(p) <br>
range: {0, 1, 2, …}</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-90">\begin{equation}
\begin{split}
pf \ \ \ f(x) & = P(X=x) \\
& = P(x \ \ \text{F's, then 1 S}) \\
 f(x) & = (1-p)^xp 
\end{split}
\end{equation}</script> <br>
no orderings to worry about since it’s just FFFF….FS (x F’s) <br>
Easy to show <script type="math/tex" id="MathJax-Element-91">\Sigma_{x=0}^\infty f(x) =1</script> using infinite geometric series formula <br>
<script type="math/tex" id="MathJax-Element-92">F(x) = P(X \leq x) = 1 - P(X \leq x+1) \\= 1 - (p(1-p)^{x+1} + p(1-p)^{x+2} + p(1-p)^{x+3} + ...)</script></p>

<table>
<thead>
<tr>
  <th></th>
  <th>Discrete Uniform</th>
  <th>Hypergeometric</th>
  <th>Bin</th>
  <th>NB</th>
  <th>Geometric</th>
  <th>Poisson</th>
</tr>
</thead>
<tbody><tr>
  <td>pf f(x)</td>
  <td><script type="math/tex" id="MathJax-Element-93">\frac{1}{b-a+1}</script></td>
  <td><script type="math/tex" id="MathJax-Element-94">\frac{{r \choose x} {N-r \choose n-x}}{{N \choose n}}</script></td>
  <td><script type="math/tex" id="MathJax-Element-95">{n \choose x}p^x(1-p)^{n-x}</script></td>
  <td><script type="math/tex" id="MathJax-Element-96">{x + k -1 \choose k-1}p^k(1-p)^x</script></td>
  <td><script type="math/tex" id="MathJax-Element-97">(1-p)^xp</script></td>
  <td><script type="math/tex" id="MathJax-Element-98">\frac{e^{-\lambda t} (\lambda t) ^x}{x!}</script></td>
</tr>
<tr>
  <td>range</td>
  <td>{a, a+1, .. ,b}</td>
  <td>weird!</td>
  <td>0,…,n</td>
  <td>0,1,…</td>
  <td>0,1 …</td>
  <td>0,1,2,…</td>
</tr>
<tr>
  <td>cdf F(x)</td>
  <td><script type="math/tex" id="MathJax-Element-99">\frac{x-a+1}{b-a+1}</script></td>
  <td>no closed form</td>
  <td>no</td>
  <td>no</td>
  <td><script type="math/tex" id="MathJax-Element-100">1-(1-p)^{x+1}</script></td>
  <td><script type="math/tex" id="MathJax-Element-101">F(x) = e^{-\lambda t} (1 + \frac{\lambda t}{1!} +\frac{(\lambda t)^2}{2!} + ... + \frac{(\lambda t)^x}{x!})</script></td>
</tr>
<tr>
  <td>when to use?</td>
  <td><script type="math/tex" id="MathJax-Element-102">- \text{fixed # values} \\ - \text{equally likely}</script></td>
  <td><script type="math/tex" id="MathJax-Element-103">- \text{w/o replacement} \\ - \text{know # S in a subset}</script></td>
  <td><script type="math/tex" id="MathJax-Element-104">- \text{Bernoulli trials - indep, S or F, P(S)} \\</script> <script type="math/tex" id="MathJax-Element-105"> -\text{know # trials} \\</script> - counting # S</td>
  <td><script type="math/tex" id="MathJax-Element-106">- \text{Bernoulli trials - indep, S or F, P(S)} \\</script> <script type="math/tex" id="MathJax-Element-107">- \text{ know # S's $\\$} \\ -\text{"before" "until" "waiting for"}</script></td>
  <td><script type="math/tex" id="MathJax-Element-108">- \text{Bernoulli trials - indep, S or F, P(S)} \\</script> <script type="math/tex" id="MathJax-Element-109">- \text{same but k=1}</script></td>
  <td>Bin with large n, small p (approx)</td>
</tr>
</tbody></table>


<p>f(x) <br>
When to use? = Bin with large n, small p (approx) <br>
range 0,1,2,…</p>



<h2 id="lecture-15">Lecture - 15</h2>

<blockquote>
  <p>Recall: uniform, hypergeometric, binomial, NB, geometric <br>
  Poisson dist from Bin (5.7) <br>
  Poisson process (5.8)</p>
</blockquote>



<h3 id="poisson-rv-57">Poisson rv (5.7)</h3>

<p>We say X has a Poisson distribution with parameter <script type="math/tex" id="MathJax-Element-110">\mu</script> (<script type="math/tex" id="MathJax-Element-111">X\sim Poi(\mu))</script> if <script type="math/tex" id="MathJax-Element-112">f(x) = \frac{e^{-\mu}\mu^x}{x!}</script> for <script type="math/tex" id="MathJax-Element-113">x=0,1,2,...</script> <br>
Easy to show <script type="math/tex" id="MathJax-Element-114">\Sigma_{x=0}^{\infty}f(x)=1</script> since <script type="math/tex" id="MathJax-Element-115">e^\mu=\Sigma_{x=0}^{\infty} \frac{{\mu}^x}{x!}</script> <br>
The Poisson is a limiting case of the Binomial when n-&gt;<script type="math/tex" id="MathJax-Element-116">\infty</script> and p-&gt;0 such that the product np remains constant  <br>
<script type="math/tex" id="MathJax-Element-117">f(x) = {n \choose x}p^x (1-p)^{n-x}</script> <br>
Let np = <script type="math/tex" id="MathJax-Element-118">\mu</script> <br>
then,  <br>
<script type="math/tex; mode=display" id="MathJax-Element-119">\begin{equation}
\begin{split}
p &=\frac{\mu}{n} \\
 &= \frac{n(n-1)...(n-x+1)}{x!}\frac{\mu}{n}^x(1-\frac{\mu}{n})^{n-x} \\
&=\frac{\mu}{x!}^x\frac{n}{n}\frac{n-1}{n}...\frac{n-x+1}{n}(1-\frac{\mu}{n})^n(1-\frac{\mu}{n})^{-x} \\
\text{now let n} \rightarrow \infty \\
&=\frac{\mu^x}{x!}*1*1...1*e^{-\mu}*1 \\
&=\frac{e^{-\mu}\mu^x}{x!} \text{ which is } Poi(\mu=np)
\end{split}
\end{equation}</script> <br>
So if we have a Bin(n,p) with large n and small p, we can approx if with a Poisson rv with parameter <script type="math/tex" id="MathJax-Element-120">\mu = np</script>. Guideline: <script type="math/tex" id="MathJax-Element-121">n\geq40</script> and <script type="math/tex" id="MathJax-Element-122">p\leq 0.05</script> works well!</p>

<blockquote>
  <p>Example: <br>
  Roll up the Rim - “1 in 9 cups win!” You buy 100 cups (treat as independent). Find prob you get 10 or fewer winning cups.</p>
</blockquote>

<p>X= # winning <br>
<script type="math/tex" id="MathJax-Element-123">X \sim Bin(100,\frac{1}{9})</script> <br>
So: <br>
<script type="math/tex; mode=display" id="MathJax-Element-124">\begin{equation}
\begin{split}
P(X\leq 10) & = f(0) + f(1) + ... + f(10) \\
& = {100 \choose 0} \frac{1}{9}^0\frac{8}{9}^100 + {100 \choose 1} \frac{1}{9}^1\frac{8}{9}^99 + ... + \text{tedious calc} \\
&=0.439
\end{split}
\end{equation}</script></p>

<p>Try Poisson approx <script type="math/tex" id="MathJax-Element-125">\mu = 100*\frac{1}{9} = 11.111</script> <br>
<script type="math/tex" id="MathJax-Element-126">Y \sim Poi(11.111)</script> <br>
<script type="math/tex" id="MathJax-Element-127">P(Y\leq 10) = \frac{e^{-11.1}}{0!}11.1^0+\frac{e^{-11.1}}{1!}11.1^1 + ... + \frac{e^{-11.1}}{11!}11.1^{10} \\= e^{-11.1}(1 + 11.1 + \frac{11.1}{2!}^2 + ... + \frac{11.1}{10!}^10) = 0.447</script></p>

<p>Not a great approx since <script type="math/tex" id="MathJax-Element-128">p = \frac{1}{9}</script> was a bit too high to be “close to 0”</p>

<blockquote>
  <p>Clicker question <br>
  Suppose you type at exactly 90 words pm and on each word have a 1% chance of making an error. After 1 minute, what is the probability you have made NO errors? <br>
  0.405 -&gt; from bin. 0.407 -&gt; from poisson</p>
</blockquote>

<p>You can also use Poisson when <script type="math/tex" id="MathJax-Element-129">p\approx 1</script> by instead modelling the number of F’s instead of S’s.</p>



<h3 id="poisson-process-58">Poisson Process (5.8)</h3>

<p>Consider “events” occurring randomly throughout time/space according to 3 conditions:</p>

<ol>
<li>Independence  <br>
<ol><li>(events have no impact on each other)</li>
<li># of events in non-overlapping time intervals are indep.</li></ol></li>
<li>Individuality  <br>
<ol><li>(events occur one at a time)</li>
<li>Cannot have two or more events at the exact same time</li></ol></li>
<li>Homogeneity/Uniformity <br>
<ol><li>(events occur at a constant rate <script type="math/tex" id="MathJax-Element-130">\lambda</script>)</li>
<li>Prob of an event occurring in a short time interval (t, t+ <script type="math/tex" id="MathJax-Element-131">\Delta</script>t) is proportional to <script type="math/tex" id="MathJax-Element-132">\lambda \Delta t</script></li>
<li>Can’t have periods of higher activity</li></ol></li>
</ol>

<blockquote>
  <p>E.g. emails into an inbox <br>
  Cars through an intersection <br>
  Births in a large population</p>
</blockquote>



<h2 id="lecture-16">Lecture - 16</h2>

<p>Imagine we observe a Poisson process (with rate <script type="math/tex" id="MathJax-Element-133">\lambda</script>) for t units of time. <br>
Let X = # of events that occur. <br>
X is a discrete rv with no maximum. <br>
It turns out that: <br>
X ~ Poi(<script type="math/tex" id="MathJax-Element-134">\mu</script> = Xt) <br>
ie. <script type="math/tex" id="MathJax-Element-135">f(x) = \frac{e^{-\lambda t} (\lambda t) ^x}{x!}</script> for x = 0, 1, 2</p>

<blockquote>
  <p>Proof: See course notes <br>
  Add one more column to your chart!</p>
</blockquote>

<p>| Poisson <br>
f(x) | <script type="math/tex" id="MathJax-Element-136">\frac{e^{-\lambda t} (\lambda t) ^x}{x!}</script> | <script type="math/tex" id="MathJax-Element-137">F(x) = e^{-\lambda t} (1 + \frac{\lambda t}{1!} +\frac{(\lambda t)^2}{2!} + ... + \frac{(\lambda t)^x}{x!})</script> <br>
When to use? = Bin with large n, small p (approx) <br>
range 0,1,2,…</p>

<ul>
<li>Conditions of a Poisson process, counting # of events in a fixed time period</li>
</ul>

<p>When NOT to use? <br>
We can specify a maximum <br>
If it makes sense to ask how many time the event did not occur</p>

<p>Example:</p>

<blockquote>
  <p>requests to a web server follow the conditions of a Poisson process with rate 100 per minute. <br>
  Find prob of 1 request in 1 sec. 90 requests in 1 min.</p>
</blockquote>

<p>Let X = # requests in 1 sec <br>
X ~ Poi(<script type="math/tex" id="MathJax-Element-138">\mu = 100 * \frac{1}{60}</script> or <script type="math/tex" id="MathJax-Element-139">\frac{100}{60} = \frac{5}{3}</script>) <br>
<script type="math/tex" id="MathJax-Element-140">P(X=1)=\frac{e^{-\frac{5}{3}}(\frac{5}{3})}{1!} = 0.314</script> <br>
Let Y = # requrests in 1 min <br>
Y ~ Poi(<script type="math/tex" id="MathJax-Element-141">\mu = 100*1</script>) <br>
P(Y=90) = <script type="math/tex" id="MathJax-Element-142">\frac{e^{-100}100^{90}}{90!} = 0.025</script></p>



<h3 id="combining-models-59">Combining Models (5.9)</h3>

<p>Many problems may combine more than one distribution together. Your task is to identify the distribution needed, depending on the perobability requested.</p>

<blockquote>
  <p>Example: Server requests 100/min <br>
  A 1-second period is “quiet” if it contains no requests. </p>
</blockquote>

<p>a) Find the Prob of a “quiet” second.  <br>
X=# requests in 1 sec. X~Poi(<script type="math/tex" id="MathJax-Element-143">\frac{5}{3}</script>) <br>
P(X = 0) = <script type="math/tex" id="MathJax-Element-144">\frac{e^{-\frac{5}{3}}(\frac{5}{3})^0}{0!} = 0.189</script></p>

<p>b) Prob of 10 “quiet” seconds in a minute (60 non-overlapping sec) <br>
Let Y = # “quiet” in 60 sec. <br>
Y~Bin(60,0.189) <br>
P(Y=10) = <script type="math/tex" id="MathJax-Element-145">{60 \choose 10}0.189^{10}0.811^{50} = 0.124</script></p>

<p>c) Prob of having to wait 30 non-overlapping sec to get 2 “quiet” <br>
Let Z = # non-quiet sec before 2 “quiet” <br>
Z ~ NB(2, 0.189) <br>
P(Z = 28) = <script type="math/tex" id="MathJax-Element-146">{28 + 2 -1 \choose 28}0.189^2 0.811^{28} = 0.003</script></p>

<p>d) Given (c), the prob there is 1 “quiet” sec in the first 15 sec. <br>
P(1 Q in 15 sec | wait 30 for 2 Q) <br>
<script type="math/tex" id="MathJax-Element-147">= \frac{P(\text{1 Q in 15 sec AND wait 30 for 2Q})}{P(\text{wait 30 for 2 Q})}</script> <br>
<script type="math/tex" id="MathJax-Element-148">= \frac{P(\text{1 Q in 15 sec})P(\text{wait 15 more to get one more Q)}}{P(\text{wait 30 for 2 Q})}</script> <br>
Use binomial for 1st prob. Geometric for 2nd. <br>
<script type="math/tex" id="MathJax-Element-149">= \frac{({15 \choose 1}0.189^10.811^{14})(0.811^{14}0.189)}{{29 \choose 28}0.189^{2}0.811^{28}}</script></p>



<h2 id="lecture-17">Lecture 17</h2>



<h3 id="ch-7-expected-value-variance-summarizing-data">Ch 7. Expected Value + Variance Summarizing Data</h3>

<p>Let X = # of kids in a family</p>

<p>To summarize the data, we can use: <br>
1. A frequency distribution</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>frequency</th>
</tr>
</thead>
<tbody><tr>
  <td>1</td>
  <td>10</td>
</tr>
<tr>
  <td>2</td>
  <td>14</td>
</tr>
<tr>
  <td>3</td>
  <td>10</td>
</tr>
<tr>
  <td>4</td>
  <td>1</td>
</tr>
<tr>
  <td>5</td>
  <td>2</td>
</tr>
</tbody></table>


<p>2. a frequency histogram (different graph here because I was lazy and didn’t want to draw one in ms paint) <br>
<img src="https://www.mathsisfun.com/data/images/bar-graph-fruit.gif"> <br>
3. A single number representing the average or sample mean <br>
<script type="math/tex; mode=display" id="MathJax-Element-150">\begin{equation}
\begin{split}
\bar x &= \frac{\text{total # kids}}{\text{# families}}\\
& = \frac{1*10 + 2*14 + 3*10 + 4*1 + 5*2}{37}\\
& = \frac{82}{37}  \\
& = 2.216
\end{split}
\end{equation}</script> <br>
4. median - the middle value: 2 in this case <br>
5. mode - most common/frequent value (in this case, 2)</p>

<p>Expectation of a r.v. (7.2) <br>
We had the sample mean of the kids be <script type="math/tex" id="MathJax-Element-151">\bar x</script> <br>
<script type="math/tex" id="MathJax-Element-152">\Sigma_{x=1}^{5} (\text{relative frequency of answering x})</script></p>

<p>We can replace the observed relative frequency with a theoretical probability of the r.v. equally x -&gt; theoretical mean</p>

<p>2011 census: </p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>1</th>
  <th>2</th>
  <th>3</th>
  <th>4</th>
  <th>5</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>0.43</td>
  <td>0.4</td>
  <td>0.12</td>
  <td>0.04</td>
  <td>0.01</td>
</tr>
</tbody></table>


<p>so the theoretical mean of X is: <br>
<script type="math/tex" id="MathJax-Element-153">\Sigma_{x=1}^{5}xf(x) = 1* 0.43 + 2*0.4 + 3*0.12 + 4*0.04 + 5*0.01 = 1.8</script> <br>
and median is 2 since F(2) &gt; 0.5 and F(1) &lt; 0.5 <br>
and mode is 1</p>

<p>Def: The expected value or expectation or mean of a discrete r.v X is: <br>
<script type="math/tex" id="MathJax-Element-154">E[X] = \mu = \Sigma_{\text{all x } \epsilon \text{ range of X}}xf(x)</script></p>



<h2 id="lecture-18">Lecture 18</h2>

<blockquote>
  <p>Recall - Expected value (aka mean) of X is <script type="math/tex" id="MathJax-Element-155">E[X]=\mu=\Sigma_{\text{all  }\epsilon \text{ range of X}}xf(x)</script></p>
</blockquote>

<p>Can think of it as a weighted average of the values X can take with weights = probabilities or balance point of the histogram of f(x). Often we may be interested in the average value of the function of x. Eg. x = usage on phone. g(X) = cost of that usage.</p>

<p>Def: the expected value of g(X) for a discrete r.v. X is <script type="math/tex" id="MathJax-Element-156">E[g(X)] = \Sigma_{\text{all  }\epsilon \text{ range of X}}g(x)f(x)</script>  <br>
weighted average of the g(x) values that can occur. (e.g. g(X) = 1000 + 250x)</p>

<p>What if <script type="math/tex" id="MathJax-Element-157">g(X) = \frac{2000}{X}</script> ? <br>
Here, <script type="math/tex" id="MathJax-Element-158">E[g(X)] \neq g(E[x])</script> because g(x) is <strong>non-linear</strong> function of X. Expectation is a linear operator.</p>

<p>SO ONLY USE IF g(X) IS LINEAR <br>
Also: <br>
<script type="math/tex" id="MathJax-Element-159">E[aX + b] = aE[X] + b</script></p>



<h3 id="applications-of-expectation-53">Applications of Expectation (5.3)</h3>

<p>If we have the distribution of X ans we let Y = g(X), we can find E[Y] by either: <script type="math/tex" id="MathJax-Element-160">E[g(X)] = \Sigma_{\text{all x}}g(x)f(x)</script> or by finding the range and pf of Y and using <script type="math/tex" id="MathJax-Element-161">\Sigma_{\text{all y}}yf(y)</script></p>

<blockquote>
  <p>Example <br>
  Suppose the time to finish a part on a coding question is 10 minutes if you make no errors. Syntax errors take 2 mins to fix. Logic errors take 10 min. Assume O(syntax error) = 0.1 , P(logic error) = 0.2 independently.</p>
</blockquote>

<p>Find the average(expected) time to finish this question. <br>
Let X = 0 (no errors), 1(syntax), 2(logic), 3(both),</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>0</th>
  <th>1</th>
  <th>2</th>
  <th>3</th>
</tr>
</thead>
<tbody><tr>
  <td>f(y)  -&gt; f(x)</td>
  <td>0.72 =(0.9*0.8)</td>
  <td>(0.1)*(0.8) = .08</td>
  <td>(0.9)*(0.2)=0.18</td>
  <td>0.02</td>
</tr>
<tr>
  <td>y -&gt; g(x)</td>
  <td>10</td>
  <td>12</td>
  <td>20</td>
  <td>22</td>
</tr>
</tbody></table>


<p>So <script type="math/tex" id="MathJax-Element-162">E[g(X)] = \Sigma_{x=0}^{3}g(x)f(x) \\ = 10*0.72 + 12*0.08 + 20*0.18 + 22*0.02 = 12.2</script></p>

<p>Example: <br>
A web server has a cache. 20% chance that the request is found in the cache (cache hit) -&gt; 10 ms. If it’s not found (cache miss) then it takes 50(send msg) + 70(lookup) + 50(return answer) ms. Find the expected time with and without the cache. <br>
Without: time is always 190ms So E[T] = 170. <br>
With: Let X = 0 if found, 1 if not found</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>0</th>
  <th>1</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>0.2</td>
  <td>0.8</td>
</tr>
<tr>
  <td>time=g(x)</td>
  <td>10</td>
  <td>180</td>
</tr>
</tbody></table>




<h2 id="lecture-19">Lecture 19</h2>

<p>Recall: <br>
<script type="math/tex; mode=display" id="MathJax-Element-163">\begin{equation}
\begin{split}
E[g(X)] &= \Sigma_{\text{all x}}g(x)f(x)\\
E[aX + b] &= aE[X]+b \text{(linear operator)}
\end{split}
\end{equation}</script></p>



<h3 id="means-and-variances-of-named-distributions-74">Means (and variances) of named distributions (7.4)</h3>

<ol>
<li>Binomial <br>
<ul><li>Let X~ Bin(n,i) <br>
Find E[X]: <br>
<script type="math/tex" id="MathJax-Element-164">E[X] = \Sigma_{allx}xf(x) \\ = \Sigma_{x=0}^{n}x{n \choose x}p^x(1-p)^{n-x} \\ = \Sigma_{x=1}^{n}x{n \choose x}p^x(1-p)^{n-x}</script> <br>
since the x=0 term is 0 <br>
<script type="math/tex" id="MathJax-Element-165">=\Sigma_{x=1}^{n}x\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} </script> <br>
Note: If <script type="math/tex" id="MathJax-Element-166">x \leq 1</script>, then <script type="math/tex" id="MathJax-Element-167">x! = x(x-1)!</script> <br>
<script type="math/tex" id="MathJax-Element-168">= \Sigma_{x=1}^{n}\frac{n!}{(x-1)!(n-x)!}p^x(1-p)^{n-x}</script> <br>
<script type="math/tex" id="MathJax-Element-169">= \Sigma_{x=1}^{n}\frac{n(n-1)!}{(x-1)!((n-1)-(x-1))!}pp^{x-1}(1-p)^{(x-1)-(x-1)}</script> <br>
<script type="math/tex" id="MathJax-Element-170">= np\Sigma_{x=1}^{n}{n-1 \choose x-1}p^{x-1}(1-p)^{(n-1)-(x-1)}</script> <br>
Let <script type="math/tex" id="MathJax-Element-171">y = x-1</script> <br>
<script type="math/tex" id="MathJax-Element-172">np\Sigma_{y=0}^{n-1}{n-1 \choose y}p^y(1-p)^{n-1-y}</script> <br>
The summation is just f(y) for Y~Bin(n-1,y) and that = 1 <br>
<script type="math/tex" id="MathJax-Element-173">\therefore E[X] = np</script> for X~Bin(n,p) <br>
This makes logical sense because the number of successes is proportional to both the # of trials and the prob of success. So the average # S is the # trials x prob of S.</li></ul></li>
<li>Poisson <br>
<ul><li>Let X~Poi(<script type="math/tex" id="MathJax-Element-174">\mu</script>) (where <script type="math/tex" id="MathJax-Element-175">\mu</script> comes from <script type="math/tex" id="MathJax-Element-176">\lambda t</script> or np or given) <br>
Find E[X] <br>
<script type="math/tex" id="MathJax-Element-177">E[X] = \Sigma_{x=0}^{\infty}x\frac{e^{-\mu}\mu^x}{x!}</script> <br>
again the x=0 term is 0 and for <script type="math/tex" id="MathJax-Element-178">x \leq 1</script>, x! =x(x-1)! <br>
<script type="math/tex" id="MathJax-Element-179">= \Sigma_{x=1}^{\infty}\frac{e^{-\mu}\mu^x}{(x-1)!}</script> <br>
<script type="math/tex" id="MathJax-Element-180">= \mu\Sigma_{x=1}^{\infty}\frac{e^{-\mu}\mu^{x-1}}{(x-1)!}</script> <br>
Let y = x-1 <br>
<script type="math/tex" id="MathJax-Element-181">= \mu \Sigma_{y=0}^{\infty}\frac{e^{-\mu}\mu^y}{y!}</script> <br>
that sum is just f(y) for Y~Poi(<script type="math/tex" id="MathJax-Element-182">\mu</script>) so <script type="math/tex" id="MathJax-Element-183">\Sigma_{y=1}^{\infty}f(y)=1</script> <br>
<script type="math/tex" id="MathJax-Element-184">\therefore E[X] = \mu</script> for X~Poi(<script type="math/tex" id="MathJax-Element-185">\mu</script>) <br>
i.e. <script type="math/tex" id="MathJax-Element-186">\mu</script> is the parameter and the mean. <br>
This makes sense if <script type="math/tex" id="MathJax-Element-187">\mu = \lambda t</script> since avg # events in t units of time should be the rate (<script type="math/tex" id="MathJax-Element-188">\frac{\text{avg # events}}{\text{time unit}}</script>) multiplied by the length of the time interval we observe.</li></ul></li>
<li>Similarly: <br>
<ol><li>X~NB(k,p) , <script type="math/tex" id="MathJax-Element-189">E[X] = \frac{k(1-p)}{p}</script></li>
<li>X~Hyp(N,r,n) , <script type="math/tex" id="MathJax-Element-190">E[X] = \frac{nr}{N}</script></li>
<li>X~DU[a,b] , <script type="math/tex" id="MathJax-Element-191">E[X] = \frac{a+b}{2}</script> <br>
these results can be proven from first principles (<script type="math/tex" id="MathJax-Element-192">\Sigma_{\text{all x}}xf(x)</script>) but there are other easier ways to show them</li></ol></li>
</ol>

<p>The mean of X, E[X] tells us where the distribution is centered, on average. But the practice we also care about how widely spread out the distribution is around that mean. <br>
E.g. determining the number of servers for an online system. need to know spread. <br>
How to measure? <br>
<script type="math/tex" id="MathJax-Element-193">E[X-\mu] = 0 </script> (not helpful!) <br>
<script type="math/tex" id="MathJax-Element-194">E[|X-\mu|]</script> - average absolute distance from mean</p>

<ul>
<li>need cases to evaluate</li>
<li>point of non-differentiabililty at <script type="math/tex" id="MathJax-Element-195">\mu</script></li>
</ul>

<p>So we use: <br>
<script type="math/tex" id="MathJax-Element-196">E[(X-\mu)^2]</script> - average squared distance from mean</p>



<h2 id="lecture-20">Lecture 20</h2>

<blockquote>
  <p>Def: Variance of a r.v. X is <script type="math/tex" id="MathJax-Element-197">Var(X)= \sigma^2 = E[(X-\mu)^2]</script> <br>
  Weighted average squared, distance from mean. If X is discrete, <script type="math/tex" id="MathJax-Element-198">Var(X) = \Sigma_{\text{all x}}(x-\mu)^2f(x)</script></p>
</blockquote>

<p>Calc form of variance: <br>
<script type="math/tex; mode=display" id="MathJax-Element-199">\begin{equation}
\begin{split}
Var(X) &= E[(X-\mu)^2] \\
&= E[X^2 - 2\mu X + \mu^2] \\
&= E[X^2] - 2\mu E[X] + \mu^2 \text{ by linearity} \\
&= E[X^2] - 2\mu^2 + \mu^2 \text{ since }E[X] = \mu \\
&= E[X^2] - \mu^2 \\
&= E[X^2] - E[X]^2
\end{split}
\end{equation}</script> <br>
Where <script type="math/tex" id="MathJax-Element-200">E[X^2] = \Sigma_{\text{all x}}xf(x)</script></p>

<p>Example: <br>
X has pf</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>10</th>
  <th>12</th>
  <th>20</th>
  <th>22</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>0.72</td>
  <td>0.08</td>
  <td>0.18</td>
  <td>0.62</td>
</tr>
</tbody></table>


<p>We found <script type="math/tex" id="MathJax-Element-201">E[X] = 12.2</script> min <br>
Now <script type="math/tex" id="MathJax-Element-202">E[X^2] = 10^2 *0.72 + 12^2*0.08 + 20^2*0.18 + 22^2*0.02 = 165.2</script> <br>
<script type="math/tex" id="MathJax-Element-203">\therefore Var(X) = 165.2 - 12.2^2 = 16.36 \text{ min}^2</script></p>

<blockquote>
  <p>Note that variance is measured in units squared rather than the original units of X. Taking the square root of the variance makes more sense.</p>
</blockquote>

<p><strong>Defn</strong>: the standard deviation of X is <script type="math/tex" id="MathJax-Element-204">SD(X) = \sigma = \sqrt{Var(X)}</script></p>

<p>In our example, <script type="math/tex" id="MathJax-Element-205">SD(X) = \sqrt{16.36} = 4.04</script> min <br>
Remember, Var(X) will always be non-negative because it’s a weighted average of <script type="math/tex" id="MathJax-Element-206">(x-\mu)^2</script> values <script type="math/tex" id="MathJax-Element-207">\geq 0</script> <br>
So SD(X) will also be <script type="math/tex" id="MathJax-Element-208">\geq 0</script> and a Real number</p>



<h3 id="mean-var-of-a-linear-fn-of-x">Mean + Var of a linear f’n of X</h3>

<p><script type="math/tex" id="MathJax-Element-209">Y=aX+b</script> <br>
<script type="math/tex" id="MathJax-Element-210">E[Y] = aE[X] + b</script> by linearity of expectations <br>
<script type="math/tex; mode=display" id="MathJax-Element-211">\begin{equation}
\begin{split}
Var(Y) &= E[(Y - E[Y])^2] \\
& = E[(aX + b - (aE[X] + b))^2] \\
&= E[a^2(X-E[X])^2] \\
&= a^2Var(X)
\end{split}
\end{equation}</script></p>

<blockquote>
  <p>Note: b does not affect the variance since shifting the distribution doesn’t affect the spread. We increase all the distances by a factor of <script type="math/tex" id="MathJax-Element-212">a</script> so the squared distances increase by <script type="math/tex" id="MathJax-Element-213">a^2</script></p>
</blockquote>

<p>Finally, <br>
<script type="math/tex" id="MathJax-Element-214">SD(Y) = \sqrt{Var{Y}} \\ = \sqrt{a^2Var(X)} \\ =|a|SD(x)</script></p>

<p>Example <br>
Suppose X has pf</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>0</th>
  <th>1</th>
  <th>2</th>
  <th>3</th>
  <th>4</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>0.1</td>
  <td>0.1</td>
  <td>0.1</td>
  <td>0.5</td>
  <td>0.2</td>
</tr>
<tr>
  <td>y</td>
  <td>1</td>
  <td>3</td>
  <td>5</td>
  <td>7</td>
  <td>9</td>
</tr>
</tbody></table>


<p>Let <script type="math/tex" id="MathJax-Element-215">Y = 2X + 1</script> <br>
<script type="math/tex" id="MathJax-Element-216">E[X] = 2.6</script> <br>
<script type="math/tex" id="MathJax-Element-217">E[X^2] = 8.2</script>  <br>
<script type="math/tex" id="MathJax-Element-218">E[Y] = 6.2</script>  <br>
<script type="math/tex" id="MathJax-Element-219">E[Y^2] = 44.2</script></p>

<p><script type="math/tex" id="MathJax-Element-220">Var(X) = E[X^2] - E[X] = 1.44</script> <br>
<script type="math/tex" id="MathJax-Element-221">Var(Y) = E[Y^2] - E[Y] = 5.76</script></p>

<p>But! We can calculate the Var(Y) in a different way! <br>
Verify <script type="math/tex" id="MathJax-Element-222">E[Y] = 2E[X]+1</script> and <script type="math/tex" id="MathJax-Element-223">Var(Y) = 2^2Var(X)</script></p>



<h3 id="variances-of-named-distributions">Variances of named distributions</h3>

<p></p><ol> <br>
<li>Poissson - X~Poi(<script type="math/tex" id="MathJax-Element-224">\mu</script>) , <script type="math/tex" id="MathJax-Element-225">E[X] = \mu</script> <br>
<ul><li><script type="math/tex" id="MathJax-Element-226">Var(X) = E[X^2] - E[X]</script> <br>
<script type="math/tex" id="MathJax-Element-227">= E[X(X-1) + X] - E[X]^2</script> <br>
<script type="math/tex" id="MathJax-Element-228">= E[X(X-1)] +E[X] - E[X]^2</script></li>
<li>Why? </li>
<li><script type="math/tex" id="MathJax-Element-229">E[X(X-1)] = \Sigma_{x=0}^{\infty}x(x-1)\frac{e^{-\mu}\mu^x}{x(x-1)(x-2)!} \\=\mu^2\Sigma_{y=0}^{\infty}\frac{e^{-\mu}\mu^y}{y!} = ty = x-2</script></li>
<li>So <script type="math/tex" id="MathJax-Element-230">Var(X) = \mu^2 + \mu - \mu^2 = \mu</script> <br>
In the Poisson, <script type="math/tex" id="MathJax-Element-231">\mu</script> is the mean and the variance!  <br>
<strong>^ the Poisson variance is a good exam question</strong></li></ul></li>
<li>Binomial - <script type="math/tex" id="MathJax-Element-232">E[X] = np</script> ,  <script type="math/tex" id="MathJax-Element-233">E[X(X-1)] = n(n-1)p^2</script> <br>
<ul><li><script type="math/tex" id="MathJax-Element-234">\therefore Var(X) = n(n-1)p^2 + np - (np)^2 = np(1-p)</script> <br></li></ul></li></ol><p></p>

<blockquote>
  <p>Note: Var is smaller than mean when <script type="math/tex" id="MathJax-Element-235">p \approx 0</script> or <script type="math/tex" id="MathJax-Element-236">p \approx 1</script> the variance is small since there’s less uncertainty
  </p>
</blockquote></body>
</html>

<!--
2: Using highlight.js
<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>STAT 230</title>
<link rel="stylesheet" type="text/css" href="https://www.dropbox.com/s/atz4kqwayz4hlhk/markDown.css?raw=1">
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.0.0/styles/agate.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.0.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script> 
</head>
<body><h1 id="stat-230-probability">STAT 230 - Probability</h1>

<p><div class="toc">
<ul>
<li><a href="#stat-230-probability">STAT 230 - Probability</a><ul>
<li><a href="#lecture-1">Lecture - 1</a><ul>
<li><a href="#the-3-definitions-of-probability">The 3 Definitions of Probability</a></li>
<li><a href="#how-does-prob-relate-to-cs">How does prob relate to CS?</a></li>
</ul>
</li>
<li><a href="#lecture-2-sept-12">Lecture 2 - Sept 12</a><ul>
<li><a href="#definitions">Definitions</a></li>
</ul>
</li>
<li><a href="#lecture-3">Lecture 3</a><ul>
<li><a href="#counting-techniques">Counting Techniques</a></li>
<li><a href="#sampling">Sampling</a></li>
<li><a href="#permutations">Permutations</a></li>
</ul>
</li>
<li><a href="#lecture-4">Lecture - 4</a><ul>
<li><a href="#combinations">Combinations</a></li>
<li><a href="#pascals-identity">Pascal’s Identity</a></li>
</ul>
</li>
<li><a href="#lecture-5">Lecture - 5</a><ul>
<li><a href="#arrangements-where-some-objects-are-alike-indistinguishable">Arrangements, where some objects are alike (indistinguishable)</a></li>
</ul>
</li>
<li><a href="#lecture-6">Lecture - 6</a><ul>
<li><a href="#de-morgans-laws">De Morgan’s Laws</a></li>
</ul>
</li>
<li><a href="#lecture-7">Lecture - 7</a><ul>
<li><a href="#multiplication-rule">Multiplication Rule</a></li>
</ul>
</li>
<li><a href="#lecture-8">Lecture - 8</a><ul>
<li><a href="#product-rule-45">Product Rule (4.5)</a></li>
<li><a href="#law-of-total-probability">Law of Total Probability</a></li>
</ul>
</li>
<li><a href="#lecture-9">Lecture - 9</a><ul>
<li><a href="#bayes-rule">Bayes’ Rule</a></li>
<li><a href="#machine-learning">Machine Learning</a></li>
<li><a href="#monty-hall-problem">“Monty Hall” problem</a></li>
<li><a href="#important-sums">Important Sums</a></li>
</ul>
</li>
<li><a href="#lecture-10">Lecture - 10</a><ul>
<li><a href="#ch-5-random-variables">Ch 5 Random Variables</a></li>
</ul>
</li>
<li><a href="#lecture-11">Lecture - 11</a><ul>
<li><a href="#properties-of-fx">Properties of F(x)</a></li>
<li><a href="#discrete-uniform-52">Discrete Uniform (5.2)</a></li>
</ul>
</li>
<li><a href="#lecture-12">Lecture 12</a><ul>
<li><a href="#hypergeometric-rv-53">Hypergeometric rv (5.3)</a></li>
<li><a href="#binomial-rv-54">Binomial rv (5.4)</a></li>
<li><a href="#tut-1">Tut 1</a></li>
</ul>
</li>
<li><a href="#lecture-13">Lecture 13</a><ul>
<li><a href="#bin-approx-to-hyp">Bin approx to Hyp.</a></li>
</ul>
</li>
<li><a href="#lecture-14">Lecture 14</a><ul>
<li><a href="#geometric-rv-56">Geometric rv (5.6)</a></li>
</ul>
</li>
<li><a href="#lecture-15">Lecture - 15</a><ul>
<li><a href="#poisson-rv-57">Poisson rv (5.7)</a></li>
<li><a href="#poisson-process-58">Poisson Process (5.8)</a></li>
</ul>
</li>
<li><a href="#lecture-16">Lecture - 16</a><ul>
<li><a href="#combining-models-59">Combining Models (5.9)</a></li>
</ul>
</li>
<li><a href="#lecture-17">Lecture 17</a><ul>
<li><a href="#ch-7-expected-value-variance-summarizing-data">Ch 7. Expected Value + Variance Summarizing Data</a></li>
</ul>
</li>
<li><a href="#lecture-18">Lecture 18</a><ul>
<li><a href="#applications-of-expectation-53">Applications of Expectation (5.3)</a></li>
</ul>
</li>
<li><a href="#lecture-19">Lecture 19</a><ul>
<li><a href="#means-and-variances-of-named-distributions-74">Means (and variances) of named distributions (7.4)</a></li>
</ul>
</li>
<li><a href="#lecture-20">Lecture 20</a><ul>
<li><a href="#mean-var-of-a-linear-fn-of-x">Mean + Var of a linear f’n of X</a></li>
<li><a href="#variances-of-named-distributions">Variances of named distributions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</p>



<h2 id="lecture-1">Lecture - 1</h2>

<blockquote>
  <p>Lecturer: Diana Skrzydlo (Skryzlo) <br>
   Office Hours: MF 3:30-5 in M3 3106 &amp; W 11:30-12:30 in M3 3144</p>
</blockquote>

<p>What is probability? <br>
The likelihood of an event occuring</p>



<h3 id="the-3-definitions-of-probability">The 3 Definitions of Probability</h3>

<ol>
<li>Classical - Probability is equal to <script type="math/tex" id="MathJax-Element-1">\frac{\text{# of ways event occurs}}{\text{total # of outcomes}}</script> (relies on outcomes being equally likely)</li>
<li>Relative Frequency - Probability = proportion of time (<script type="math/tex" id="MathJax-Element-2">\infty</script> long) that the event occurs</li>
<li>Subjective - Probability = how certain the person is that it will occur</li>
</ol>



<h3 id="how-does-prob-relate-to-cs">How does prob relate to CS?</h3>

<ul>
<li>optimizing runtime</li>
<li>randomized algos</li>
<li>confidence bounds on answers</li>
<li>recommendation engine</li>
<li>targeted ads</li>
<li>machine learning</li>
</ul>

<p>Definitions to set up a probability model</p>



<h2 id="lecture-2-sept-12">Lecture 2 - Sept 12</h2>

<ul>
<li>Definitions for probability models: experiment, trial, outcome, sample space</li>
<li>events</li>
<li>probability</li>
<li>example: binary search trees</li>
</ul>

<p>Review Question:</p>

<blockquote>
  <p>What is essential to the classical definition of probability? <br>
  Outcomes are equally likely</p>
</blockquote>



<h3 id="definitions">Definitions</h3>

<dl>
<dt>Experiment</dt>
<dd>A process that can be repeated multiple times, with multiple results</dd>

<dt>Trial</dt>
<dd>One iteration of an experiment</dd>

<dt>Outcome</dt>
<dd>The result on one trial of an experiment</dd>

<dt>Sample Space</dt>
<dd>A set of all possible outcomes for a single trial of an experiment. Notes: S is a set. i.e. an unordered, unique element list. S must contain ALL possible outcomes. Two types: discrete (finite) -&gt; can list/count, continuous -&gt; uncountably many elements

<blockquote>
  Example <br>
  Experiment: Roll a 6-sided die repeatedly <br>
  Trial: One roll <br>
  Outcomes: 1, 2, … , 5, 6 <br>
  <script type="math/tex" id="MathJax-Element-3">S_1</script> = {1, 2, 3, 4, 5, 6} (most versatile) <br>
  <script type="math/tex" id="MathJax-Element-4">S_2</script> = {even, odd} (also works)
</blockquote></dd>

<dt>Event</dt>
<dd>A subset of a sample space (including the empty set <script type="math/tex" id="MathJax-Element-5">\phi</script>) and the entire sample space <script type="math/tex" id="MathJax-Element-6">S</script>

<blockquote>
  Example <br>
  A = “a 1 is rolled” -&gt; A = {1} <br>
  B = “the number is odd” -&gt; B = {1, 3, 5} <br>
  <strong>Note:</strong> events consisting of one sample point are simple events, otherwise they are called compound events
  
  On any trial, if the outcome is in the event A, we say A has occurred. <br>
  <strong>E.g.</strong> if a 1 is rolled -&gt; A occurs, B occurs <br>
  3 is rolled -&gt; B occurs, A does not <br>
  6 is rolled -&gt; neither occur
</blockquote></dd>

<dt>A Probability</dt>
<dd>
<p>is a function that maps events in a sample space to <script type="math/tex" id="MathJax-Element-7">Real</script> numbers such that </p>

<ol>
<li><script type="math/tex" id="MathJax-Element-8">0 \leq P(A) \leq 1</script> for any A <br>
<ul><li><script type="math/tex" id="MathJax-Element-9">0 =</script> “impossible”</li>
<li><script type="math/tex" id="MathJax-Element-10">1 =</script> “guaranteed”</li></ul></li>
<li>If <script type="math/tex" id="MathJax-Element-11">a_i</script> are the elements of S, then <script type="math/tex" id="MathJax-Element-12">\Sigma_{\text{all ai}} P(a_i) = 1</script> <br>
<ul><li>where Simple event <script type="math/tex" id="MathJax-Element-13">A_i = \{a_i\}</script> </li>
<li>Example: Roll a 6-sided die twice. S = {(1,1), (1,2), … , (1,6), … , (6, 1), … , (6,6)}</li>
<li>Example: BST. A Binary Search Tree is degenerate if each node has at most one child</li>
<li>Find the prob. that a tree with 3 elements is degenerate: </li>
<li>We drew out all probs. 4/6 chance</li></ul></li>
</ol>
</dd>
</dl>



<h2 id="lecture-3">Lecture 3</h2>

<p>Recall</p>

<blockquote>
  <p>P(A) is the probability that A occurs. A is a subset of S. If A is compound, its prob is the sum of the probs of the simple events that make up A.</p>
</blockquote>



<h3 id="counting-techniques">Counting Techniques</h3>

<p>Two basic counting rules:</p>

<ol>
<li>Addition rule <br>
<ul><li>if you can do 1 task in p ways and task 2 in q ways. Then the number of ways to do task 1 XOR task 2 is just p + q.</li></ul></li>
<li>Multiplication rule <br>
<ul><li>If we can do task 1 in p ways and for each of those ways, we can do task 2 in q ways, then the total number of ways we can do task 1 AND 2 is pq.</li></ul></li>
</ol>



<h3 id="sampling">Sampling</h3>

<p>“with replacement” - is possible to obtain the same result more than once. Then it’ll have <script type="math/tex" id="MathJax-Element-14">m^k</script> choices. M spaces, k options.</p>



<h3 id="permutations">Permutations</h3>

<p>A permutation is an ordering of k objects selected from n objects without replacement. Let n = number of objects. K spaces. The order matters! <script type="math/tex" id="MathJax-Element-15">n^{(k)}</script> = “n to k factors” = <script type="math/tex" id="MathJax-Element-16">n(n-1)...(n-k+1)</script> <br>
Also <script type="math/tex" id="MathJax-Element-17">\frac{n!}{(n-k)!}</script></p>

<blockquote>
  <p>Tip, if computing a certain permutation is hard, try 1 - opposite of what you want to find. Eg. Find P(of at least one repeat) turns into 1 - P(no repeats) which is easier.</p>
</blockquote>



<h2 id="lecture-4">Lecture - 4</h2>



<h3 id="combinations">Combinations</h3>

<p>A combination is a selection of k objects from n objects without replacement. The order does NOT matter. E.g. drawing a hand of cards. Denoted by N choose K (I forget how to <script type="math/tex" id="MathJax-Element-18">\LaTeX</script> this) Pascal triangle - n choose k is the kth entry in the nth row.</p>

<p>Some shit about pascal’s triangle that is super obvious</p>



<h3 id="pascals-identity">Pascal’s Identity</h3>

<blockquote>
  <p>(n choose k) = (n-1 choose k-1) + (n-1 choose k) <br>
  We won’t prove this mathematically since it’s messy to prove</p>
</blockquote>

<p>oh lool, we proved this in MATH 239.</p>



<h2 id="lecture-5">Lecture - 5</h2>

<p>Example: <br>
A box of 10 computer chips containing 3 defective ones in the box. Four of the chips are tested. What is the probability that 1 is found defective? <br>
Solution: <br>
W/o rep &amp; don’t care about order.</p>

<p><script type="math/tex" id="MathJax-Element-19">Prob = \frac{\text{# of ways to get 1 def chips}}{\text{# of ways to test 4}} = \frac{\binom{3}{1}\binom{7}{3}}{\binom{10}{4}}</script></p>

<p>The whole time, we don’t care about order. (i.e. we don’t need to worry if the defective one is first, last or etc)</p>

<blockquote>
  <p>Note: “1 if found” means exactly 1. Not at least one.</p>
</blockquote>



<h3 id="arrangements-where-some-objects-are-alike-indistinguishable">Arrangements, where some objects are alike (indistinguishable)</h3>

<blockquote>
  <p>Can be arranged  <script type="math/tex" id="MathJax-Element-20">\frac{n!}{n_1!n_2!...n_k!}</script> where <script type="math/tex" id="MathJax-Element-21">n_1 + n_2 + ... + n_k = n</script> and <script type="math/tex" id="MathJax-Element-22">n_k</script> is number of type k.</p>
</blockquote>



<h2 id="lecture-6">Lecture - 6</h2>

<p>Recall - event -&gt; subset of S for any event A, <script type="math/tex" id="MathJax-Element-23">0 \leq P(A) \leq 1</script> <br>
Rules for probabilities <br>
If an event A is contained in another event B (A subset B) Then, <script type="math/tex" id="MathJax-Element-24">P(A) \leq P(B)</script></p>

<p><script type="math/tex" id="MathJax-Element-25">P(B) = \Sigma_{a_i \epsilon B} P(a_i)</script> <br>
<script type="math/tex" id="MathJax-Element-26">= \Sigma_{a_i \epsilon A} P(a_i) + \Sigma_{a_i \epsilon B but not A}P(a_i)</script> <br>
<script type="math/tex" id="MathJax-Element-27">= P(A) + (\geq 0)</script> <br>
thus, <script type="math/tex" id="MathJax-Element-28">P(A) \leq P(B)</script></p>

<p>A_bar , A^c, A^’, -&gt; NOT A. Aunion B = or. A intersect B = and.</p>



<h3 id="de-morgans-laws">De Morgan’s Laws</h3>

<p>Bar{aunionB} = a_bar and b_bar <br>
bar{aintersectB} = a_bar or b_bar</p>

<p>P(a union B) = P(A) + P(B) - P(AB)</p>



<h2 id="lecture-7">Lecture - 7</h2>



<h3 id="multiplication-rule">Multiplication Rule</h3>

<p>applies when events are independent of each other (don’t affect one another)</p>



<h2 id="lecture-8">Lecture - 8</h2>

<p>Recall: Independent: P(AB) = P(A)P(B) <br>
Conditional prob of A given B: P(A|B) = <script type="math/tex" id="MathJax-Element-29">\frac{P(AB)}{P(B)}</script></p>

<p>Last time we had <br>
A = small = 3 <br>
C = total = 8 <br>
P(A|C) = 1/5 <br>
P(A) = 1/6</p>

<p>C occuring makes A more likely.</p>

<p>P(C|A) = 1/6, P(C) = 5/36</p>

<p>A occurring makes C more likely. Dependence is a two-way relationship</p>

<p>Alternative defn of independence. P(A|B) = P(A) <br>
knowing B occurred has no effect on the probability of A occurring. P(B|A) = P(B)</p>

<p>Example:</p>

<p>Diana has 2 kids. Each independently has 0.5 chance of having red hair. At least one has red hair. What is the probability the other does? Let A = at least one red</p>

<table>
<thead>
<tr>
  <th></th>
  <th>Kid 1</th>
  <th>Kid 2</th>
</tr>
</thead>
<tbody><tr>
  <td><code>red</code></td>
  <td>0.25</td>
  <td>0.25</td>
</tr>
<tr>
  <td><code>not red</code></td>
  <td>0.25</td>
  <td>0.25</td>
</tr>
</tbody></table>


<p>P(A) = 0.75</p>

<p>P(both|A) = P(both and A) / P(A) = 0.25/0.75 = 1/3</p>



<h3 id="product-rule-45">Product Rule (4.5)</h3>

<p>If we rearrange the definition of conditional prob, we get: <br>
P(AB) = P(B)P(A|B) <br>
P(AB) = P(A)P(B|A)</p>

<p>Intuitively, think of “checking” whether A occurred. If it did, “check” whether B occurred (knowing A did). (or you check B first, then A) Works for both dependent and independent events.</p>

<p>Extends nicely to multiple events P(ABC) = P(B)P(C|B)P(A|BC) <br>
The order doesn’t matter as long as each probability is conditional on what we know.</p>



<h3 id="law-of-total-probability">Law of Total Probability</h3>

<p>Let <script type="math/tex" id="MathJax-Element-30">A_1 , ... , A_k</script> be a collection of events that are mutually exclusive <script type="math/tex" id="MathJax-Element-31">A_i\cap A_j = \phi</script> cover the sample space <script type="math/tex" id="MathJax-Element-32">A_1 \cup ... \cup A_k = S</script></p>

<p>(break S up into k disjoint pieces) This is a partition of S <br>
e.g. A and A_bar is a partition of size 2. <br>
For any event B in S, we can express it as B = <script type="math/tex" id="MathJax-Element-33">\cup_{i=1}^{k} (BA_i)</script> <br>
So P(B) = <script type="math/tex" id="MathJax-Element-34">\Sigma_{i=1}^{k}P(A_i)P(B|A_i)</script> using product rule</p>

<p>Example: <br>
Three people A,B,C are writing code together. A produces twice as much as B or C</p>

<p>1% of A’s code has errors <br>
2% of B’s <br>
5% of C’s <br>
What is the probability that a random line of code has errors?</p>

<p>Let E = line has error <br>
A = line written by A <br>
B = line written by B <br>
C = line written by C</p>

<p>We want P(E)</p>

<p>Using the partition {A, B, C}, P(A) = 0.5 , P(B) = P(C)  = 0.25 <br>
We have P(E|A) = 0.01 <br>
P(E|B) = 0.02 <br>
P(E|C) = 0.05</p>

<p>So <script type="math/tex" id="MathJax-Element-35">P(E) = P(A)P(E|A) + P(B)P(E|B) + P(C)P(E|C) = 0.5*0.01 + 0.25*0.02 + 0.25*0.05 = 0.0225</script></p>



<h2 id="lecture-9">Lecture - 9</h2>

<p>We found P(E) = 0.0225 using law of total prob. We would like to know, if we find an error, whose fault is it? <br>
We had to reverse the direction of the conditioning: we have P(E|author). We want P(author|E).</p>



<h3 id="bayes-rule">Bayes’ Rule</h3>

<p><script type="math/tex" id="MathJax-Element-36">P(A|B) = \frac{P(AB)}{P(B)} = \frac{P(A)P(B|A)}{P(B)}</script></p>

<p>also written as <br>
<script type="math/tex" id="MathJax-Element-37">P(A|B) = \frac{P(A)P(B|A)}{\Sigma_{i=1}^k P(A_i)P(B|A_i)}</script></p>

<p>For this example: <br>
<script type="math/tex" id="MathJax-Element-38">P(A|E) = \frac{0.5*0.01}{0.0225} = 0.222...</script></p>

<p>Example</p>

<blockquote>
  <p>Your spam filter has 90% detection rate but 1% false positive rate. Suppose 25% of messages are spam</p>
</blockquote>

<p>Let S = “message is spam” P(S) = 0.25 P(S’) = 0.75 <br>
I = “Identified as spam”  <br>
P(I/S) = 0.9 -&gt; P(I’|S) = 0.0</p>

<p>photo here&lt;&gt;</p>



<h3 id="machine-learning">Machine Learning</h3>

<p>Supervised learning -&gt; give some training data <br>
Unsupervised learning -&gt; no training data</p>

<dl>
<dt>Types of Problems</dt>
<dt>Classification</dt>
<dd>spam vs non-spam email</dd>

<dd>cancerous vs benign tumor</dd>

<dd>fraudulent vs legit banking transactions</dd>

<dt>Regression</dt>
<dd>predict the price of house/stock</dd>

<dd>length of recovery time</dd>

<dt>Clustering</dt>
<dd>grouping products together</dd>

<dd>making recommendations</dd>
</dl>

<p>Bayesian Classifier <br>
P(Category 1 | evidence) =<script type="math/tex" id="MathJax-Element-39">\frac{P(cat1)P(ev|cat1)}{P(evidence)}</script></p>



<h3 id="monty-hall-problem">“Monty Hall” problem</h3>

<p>3 doors. 1 door is revealed to be a dud. Do you switch doors?</p>

<p>Yes</p>



<h3 id="important-sums">Important Sums</h3>

<ol>
<li>Geometric - <script type="math/tex" id="MathJax-Element-40">\frac{a}{1-r}</script></li>
<li>Binomial - <script type="math/tex" id="MathJax-Element-41">(a+b)^n = \Sigma</script></li>
</ol>



<h2 id="lecture-10">Lecture - 10</h2>

<p>Bayes’ Rule:</p>

<blockquote>
  <p>P(A|B) = <script type="math/tex" id="MathJax-Element-42">\frac{P(A)P(B|A)}{P(B)}</script> <br>
  Product rule + law of total prob. A and A’ is a partition</p>
</blockquote>



<h3 id="ch-5-random-variables">Ch 5 Random Variables</h3>

<blockquote>
  <p>Def. A random variable (rv) is a function that maps points in a sample space to Real numbers</p>
</blockquote>

<p>The values that the rv can take on are called the RANGE of the rv. By convention, we call rvs X,Y,Z and the values, x,y,z <br>
We are interested in finding the prob. that a rv X takes on one of it’s particular values. ie P(X =x) (outcome is a sample point that maps to x)</p>

<p>There are two types of rvs, discrete -&gt; finite or countably infinite <strong>range</strong> vs continuous -&gt; incountably infinite <strong>range</strong></p>

<p>More than one rv can be defined on the same S</p>

<blockquote>
  <p>eg roll 3 fair 6-sided dice. <br>
  X = sum on 3 dice {3, … , 18} <br>
  Y = avg value {1, … , 6} <br>
  Z = the 3 digit number created by the dice <br>
  W = the product of the 3 {1, 2, … , 216} <br>
  U = number of 3s {1,2,3}  <br>
  etc</p>
</blockquote>

<hr>

<blockquote>
  <p>Def. The probability factor (pf) of a discrete rv X is f(x) = P(X = x) and is only defined for x <script type="math/tex" id="MathJax-Element-43">\epsilon</script> range of X. f(x) if x does not belong in range is undefined or 0.</p>
  
  <dl>
<dt>Properties of f(x)</dt>
<dd><script type="math/tex" id="MathJax-Element-44">0 \leq f(x) \leq 1</script> for all x <script type="math/tex" id="MathJax-Element-45">\epsilon</script> range of f(x) . Why? it’s a probability</dd>

<dd><script type="math/tex" id="MathJax-Element-46">\Sigma_{\text{all x}\ \epsilon \ \text{range}} f(x) = 1</script> The events “X = x” are mutually exclusive for each x</dd>
</dl>
</blockquote>

<hr>

<blockquote>
  <p>Def. The cumulative distribution function (cdf) of the rv X is F(x) = P(X <script type="math/tex" id="MathJax-Element-47">\leq</script> x) and is defined for ALL Real x.</p>
</blockquote>



<h2 id="lecture-11">Lecture - 11</h2>

<blockquote>
  <p>Recall: Random variable (X) maps sample points to Real numbers <br>
  Probability function f(x) = P(X=x) <script type="math/tex" id="MathJax-Element-48">\forall</script> x <script type="math/tex" id="MathJax-Element-49">\epsilon</script> range <br>
  Cumulative distribution function F(x) = P(<script type="math/tex" id="MathJax-Element-50">X \leq x</script>) <script type="math/tex" id="MathJax-Element-51">\forall</script> x <script type="math/tex" id="MathJax-Element-52">\epsilon R</script> </p>
</blockquote>



<h3 id="properties-of-fx">Properties of F(x)</h3>

<ol>
<li><script type="math/tex" id="MathJax-Element-53">0 \leq F(x) \leq 1</script> Why? it’s a probability</li>
<li>F(x) is non-decreasing with respect to x . Why? Can’t lose probability when increasing x <br>
<ol><li>Proof: the event “<script type="math/tex" id="MathJax-Element-54">X \leq a</script>” is contained in “<script type="math/tex" id="MathJax-Element-55">X \leq b</script>” where <script type="math/tex" id="MathJax-Element-56">a \leq b</script> so <script type="math/tex" id="MathJax-Element-57">F(a) \leq F(b)</script></li></ol></li>
<li><script type="math/tex" id="MathJax-Element-58">lim_{x \rightarrow - \infty} F(x) = 0</script> where 0 is impossible</li>
<li><script type="math/tex" id="MathJax-Element-59">lim_{x \rightarrow \infty} F(x) = 1</script> where 1 is guarenteed</li>
</ol>

<p>Example:</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>0</th>
  <th>1</th>
  <th>2</th>
  <th>3</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>125/216</td>
  <td>75/216</td>
  <td>15/216</td>
  <td>1/216</td>
</tr>
<tr>
  <td>F(x)</td>
  <td>125/216</td>
  <td>200/216</td>
  <td>215/216</td>
  <td>1</td>
</tr>
</tbody></table>


<p>Let’s graph it to show what happens at other Real values of x. <br>
<img src="http://i.stack.imgur.com/nzR9r.png"></p>

<ul>
<li>needs to approach one</li>
<li>needs to approach zero</li>
<li>right-step function (holes are filled on the left. empty on the right)</li>
<li>On this image, needs to have an arrow from 0 going to the left</li>
</ul>

<p>Relationship between F and f <br>
f(x) = the size of the jump in F(x) at the point x = F(x) - F(x-1) &lt;- which is the next smallest value in range</p>

<p><script type="math/tex" id="MathJax-Element-60">F(x) = f(0) + f(1) + ... + f(\lfloor x \rfloor) = \Sigma_{i=0}^{\lfloor x \rfloor} f(i)</script> <br>
We can use F(x) to find <script type="math/tex" id="MathJax-Element-61">P(a < X \leq b)</script> = P()</p>



<h3 id="discrete-uniform-52">Discrete Uniform (5.2)</h3>

<p>Let X be a rv on the range {a, a+1, …, b} where <script type="math/tex" id="MathJax-Element-62">a \leq b</script> &amp; a,b <script type="math/tex" id="MathJax-Element-63">\epsilon Z</script> where each value is equally likely. Then we say X has a discrete uniform distribution on [a , b] in other words, “X~DU[a,b]” . <em>X has a DU distribution of [a,b]</em></p>

<p>e.g. </p>

<ul>
<li>fair die ~DU[1,6] </li>
<li>position in a deck of one particular card ~ DU [1, 52]</li>
</ul>

<p>Find the pf f(x) = P(X = x) = c must be constant since all equal <br>
We need <script type="math/tex" id="MathJax-Element-64">\Sigma_{x = a}^{b}f(x) = 1</script> <br>
<script type="math/tex" id="MathJax-Element-65">\Sigma_{x=a}^{b}c  = 1 </script> <br>
<script type="math/tex" id="MathJax-Element-66">(b-a+1)c  = 1 </script> <br>
<script type="math/tex" id="MathJax-Element-67">C = \frac{1}{b-a+1} = f(x) </script></p>

<p>Find the cdf <br>
<script type="math/tex; mode=display" id="MathJax-Element-68">\begin{equation}
\begin{split}
F(x) & = \Sigma_{i = a}^{\lfloor x \rfloor}f(i) \\ 
& = \Sigma_{a}^{\lfloor x\rfloor} \frac{1}{b-a+1} \\
F(x) & = 
 \left\{
\begin{array}{ll}
      \frac{\lfloor x \rfloor -a + 1}{b-a+1} & a\leq x \leq b \\
      0 & x < a\\
      1 & x > b \\
\end{array} 
\right.
\end{split}
\end{equation}</script></p>

<p>Example</p>

<blockquote>
  <p>You are looking through a linked list of 100 items for one particular item. Let X = # of comparisons to find it. Claim: X ~ DU [1, 100]</p>
</blockquote>



<p><script type="math/tex; mode=display" id="MathJax-Element-69">\begin{equation}
\begin{split}
f(1) &= P(X=1) = \frac{1}{100} \\
f(2) & = P(X =2) \\
& = P(X \neq 1)P(X=2|X\neq 1) \\
& = \frac{99}{100}*\frac{1}{99} = \frac{1}{100} \\
similarly, all f(x)  &= \frac{1}{100}
\end{split}
\end{equation}</script></p>



<h2 id="lecture-12">Lecture 12</h2>



<h3 id="hypergeometric-rv-53">Hypergeometric rv (5.3)</h3>

<p>Set up: we have N objects -&gt; r “successes”, N-r “Failures” <br>
We choose n without replacement</p>

<p>Let X = “# of S’s chosen” <br>
eg. if x=# of winning numbers on a lotto 6/49 ticket <br>
X ~ Hyp(N,r,n) <br>
N=49, r = 6, n =6 <br>
Find the pf f(x) = P(X=x) = P(we get x S’s and n-x F’s)</p>

<p><script type="math/tex" id="MathJax-Element-70">f(x) = \frac{{r \choose x}{N-r \choose n-x}}{N \choose n}</script> <br>
range of x? <br>
lower bound: <script type="math/tex" id="MathJax-Element-71">x \leq 0</script>, <script type="math/tex" id="MathJax-Element-72">x \leq n - (N-r)</script> possible to run out of F’s so remaining will all be S’s</p>

<p>upper bound x \leq n (all 5’s), x \leq r (could run out of S’s) <br>
so the range of X is messy and depends on the relationship btwn N, r, and n.</p>

<p>Also, there is no closed form expression for F(x). You have to add up the f(x) values which is computationally intensive when N is large. <br>
Example: <br>
You have 10 cards - 7 treasure, 3 non-treasure. <br>
Draw 5 without replacement. Let X = “# of treasure cards”. X ~ Hyp(N=10, r=7, n=5) <br>
f(x) = P(X = x) = <script type="math/tex" id="MathJax-Element-73">\frac{{7 \choose x}{3 \choose 5-x}}{10 \choose 5}</script> for x = 2,3,4,5 <br>
All the values of f(x) will add up to 1 <br>
(proven using Hypergeometric series result)</p>



<h3 id="binomial-rv-54">Binomial rv (5.4)</h3>

<p>Set up: Bernoulli trials - independent trials, 2 outcomes on each (S or F), P(S) = P is constant for all trials</p>

<p>We do n trials. <br>
Let X = # of S in n trials <br>
(You can imagine it as a Hypergeometric selecting with replacement instead of without.) <br>
We write X ~ Bin(n, p) <br>
Eg. Flip a fair coin 10 times, x = # heads, X ~ Bin (n = 10, p = 0.5) <br>
Find the pf f(x) = P(X=x) = P(we get x S’s and n-x F’s) = p^x(1-p)^(n-x)j</p>



<h3 id="tut-1">Tut 1</h3>

<ol>
<li>The letters of PROBABILITY are arranged at random in a row. Find the probability that: <br>
<ol><li>Y is in the last position <br>
<ul><li>Total # is 9979200</li>
<li># to have Y last = 907200</li>
<li>so Prob = 1/11 (also can just consider that 1/11 chance last letter is y)</li></ul></li>
<li>The two B’s are consecutive <br>
<ul><li>treat “BB as one letter</li>
<li>P = (10!/2!)/(11!/(2!2!)) = 0.182</li></ul></li>
<li>The two B’s are consecutive and Y is in the last position <br>
<ul><li>(9!/2!)/(11!/(2!2!)) = 0.018 = P(AB)</li>
<li>Note: <script type="math/tex" id="MathJax-Element-74">P(AB) \neq P(A)P(B)</script></li></ul></li>
<li>Y is not in the last position and the two B’s are not consecutive <br>
<ul><li>P(A_bar B_bar) = P((AUB)_bar) = 1-P(AUB) = 1-(P(A)+P(B)-P(AB)) = 0.745</li></ul></li>
<li>Y is not in the last position or the two B’s are not consecutive <br>
<ul><li>P((AB)_bar) = P(A_bar U B_bar) = 1-P(AB) = 0.982</li></ul></li></ol></li>
<li>You are trying to guess your friend’s Quest password. You know it must be 8 characters chosen from digits 0-9, lower case letters a-z, and uppser case letters A-Z and is not allowed to be all letters or all numbers <br>
<ol><li>how many possible valid passwords are there? <br>
<ul><li>62 possible characters</li>
<li>62^8 - 52^8 - 10^8</li></ul></li>
<li>You happen to know that your friend is a bit lazy with respect to password security and will only use the letters a,s,d and f (both upper and lower) and the number 1. How many possible valid passwords could your friend have? <br>
<ul><li>now 1 + 4 + 4 = 9 characters</li>
<li>So 9^8  - 8^8 - 1 = 26269504</li></ul></li>
<li>What is the probability your friend’s password has no repeated characters? <br>
<ul><li>no repeats - selecting w/o replacement so order still matters </li>
<li>So 9^8 - 8^8 - 0 = 322560</li>
<li>P = 322560/26269504 = 0.012</li></ul></li>
<li>What is the probability your friend’s password contains at least one “a” or “A”? <br>
<ul><li>1 - P(no “a” or “A”s)</li>
<li># w/o = 7^8 -6^8 -1^8</li>
<li>P = 1 - (7^8 -6^8 -1^8)/( 9^8  - 8^8 - 1) = 0.844</li></ul></li></ol></li>
<li>Consider the machine learning problem of classifying incoming messages as spam. We define: A_1 = message fails rdns check (i.e. the “from” domain does match), A_2 = message is sent to over 100 people, A_3 = message contains a link with the url not matching the alt text. We will assume that the A_i’s are independent events, <strong>given</strong> that a message is spam, and that they are independent events, <strong>given</strong> that a message is regular. This is known as the “Naive Bayes Classifier” and is the simplest of the machine learning classification algorithms. We estimate P(A_1|Spam) = 0.3 ,P(A_2|Spam) = 0.2,P(A_3|Spam) = 0.1, P(A_1|Not Spam) = 0.005, P(A_2|Not Spam) = 0.04, P(A_3|Not Spam) = 0.05 and P(Spam) = 0.25 <br>
<ol><li>Suppose a message has all of features 1,2,and 3 present. Det P(Spam|A_1,A_2,A_3) <br>
<ul><li>= (P(Spam)P(A_1A_2A_3|Spam))/(P(Spam)P(A_1A_2A_3|Spam) +P(Spam_bar)P(A_1A_2A_3|Spam)) = (P(S)P(A_1|S)P(A_2|S)P(A_3|S))/(P(S)P(A_1|S)P(A_2|S)P(A_3|S) + P(S_bar)P(A_1|S)P(A_2|S)P(A_3|S)) = 0.995</li></ul></li>
<li>Suppose a message has features 1 and 2 present, but feature 3 is not present. Determine P(Spam | A_1A_2(A_3)bar). <br>
*(P(S)P(A_1|S)P(A_2|S)P(A_3_bar|S))/(P(S)P(A_1|S)P(A_2|S)P(A_3_bar|S) + P(S_bar)P(A_1|S)P(A_2|S)P(A_3_bar|S)) = 0.9895</li>
<li>If you declared as spam any message with one or more of features 1,2, or 3 present, what fraction of spam emails would you detect? <br>
<ul><li>P(A_1UA_2UA_3|Spam) = 1 - P(none of features) = 1-P(A_1_barA_2_barA_3_bar|spam) = 1- P(A_1_bar|Spam) … P(A_3_bar|Spam) = 0.496</li></ul></li>
<li>Given that a message is declared as spam (according to the rule in (c)), what is the probability that it actually is spam? <br>
<ul><li>similar to e. ans = 0.8508</li></ul></li></ol></li>
<li>Given that a message is declared as spam, (according to the rule in (c)), what is the probability that feature 1 is present? <br>
<ul><li>P(A_1|A_1UA_2UA_3) = P(A)/(P(A_1UA_2UA_3)) = P(S)P(A_1|S)+P(S_bar)P(A_2|S) )/(P(S)*0.496) = 0.641</li></ul></li>
<li>Let X represent the number of days in Feb. with temp below -24C. The probability function (pf) of X, f(x)= P(X=x) insert photo <br>
<ol><li>0.0625</li>
<li>look at one note</li>
<li>F(3.5) - F(0.5) = 0.375</li>
<li>f(2)/0.375 = 0.8333…</li></ol></li>
</ol>



<h2 id="lecture-13">Lecture 13</h2>

<blockquote>
  <p>Recall: <br>
  X~Hyp(N, r, n) <br>
  <script type="math/tex" id="MathJax-Element-75">f(x) = \frac{{r \choose x}{N-r \choose n-x}}{{N \choose n}}</script> <br>
  X = # S’s in n objects w/o rep <br>
  X~Bin(n,p) <br>
  <script type="math/tex" id="MathJax-Element-76">f(x) = {n \choose x}p^x (1-p)^{n-x}</script></p>
</blockquote>

<p>Example: <br>
Want to send a 4-bit message. Each bit is independently flipped (0-&gt;1 or 1-&gt;0) <br>
Probability = 0.1 <br>
P(message received correctly?) <br>
Let X = # of bits flipped <br>
X~Bin(4, 0.1) <br>
P(X=0) =<script type="math/tex" id="MathJax-Element-77">{4 \choose 0}0.1^00.9^4 = 0.656</script></p>

<p>Now add 3 “parity bits” to the message which allows the receiver to detect and fix up to 1 error. <br>
Let Y = # bits flipped ~Bin(7, 0.1) <br>
P(Y=0) + P(Y=1) are both ok. <br>
<script type="math/tex" id="MathJax-Element-78">= {7 \choose 0}0.1^00.9^7 + {7 \choose 1}0.1^10.9^6= 0.85</script></p>



<h3 id="bin-approx-to-hyp">Bin approx to Hyp.</h3>

<p>(ie. n &lt;&lt; N) then it doesn’t make a big difference to the probabilities if you sample with or without replacement. If we did it with replacement the number of S’s we get <br>
X~Bin(n,p=<script type="math/tex" id="MathJax-Element-79">\frac{r}{N}</script>) <br>
So when N is large and n is small, we can use a Bin(n, <script type="math/tex" id="MathJax-Element-80">\frac{r}{N}</script>) to approximate a Hyp(N,r,m). (when <script type="math/tex" id="MathJax-Element-81">\frac{n}{N}</script> is less than 0.05)</p>



<h2 id="lecture-14">Lecture 14</h2>

<blockquote>
  <p>recall: Negative Binomial (5.5) <br>
  Bernoulli trials (indep, S or F P(S)=p) X = # of F’s before the Kth S is obtained <br>
  pf and examples of NB <br>
  Geometric rv (5.6) <br>
  How to tell when to use distributions</p>
</blockquote>

<table>
<thead>
<tr>
  <th>Bin</th>
  <th>NB</th>
</tr>
</thead>
<tbody><tr>
  <td>-know # trials(n)</td>
  <td>-know # successes(k)</td>
</tr>
<tr>
  <td>-? successes modelled by X</td>
  <td>-? trails modelled by k+x</td>
</tr>
</tbody></table>


<p>We write <script type="math/tex" id="MathJax-Element-82">X\sim NB(k,p)</script> <br>
range <script type="math/tex" id="MathJax-Element-83">\{0,1,2, ...\}</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-84">\begin{equation}
\begin{split}
pf \ \ \ f(x) & = P(X=x) \\
& = P(x \ \ \text{F's befpre the kth S}) \\
& = {x+k-1 \choose k-1} p^k(1-p)^x \\
&{x+k-1 \choose k-1}  \ \text{is # orderings} \\
&p^k  \ \text{p of k S's} \\
&(1-p)^x  \ \text{p of x F's}
\end{split}
\end{equation}</script></p>

<p>We can show that <script type="math/tex" id="MathJax-Element-85">\Sigma^{\infty}_{x=0}f(x) = 1</script> <br>
But there is no closed form expression for <script type="math/tex" id="MathJax-Element-86">F(x)=\Sigma^{x}_{y=0}f(y)</script></p>

<blockquote>
  <p>Example: <br>
  A startup is looking for 5 investors. Each investor will independently say yes with probability 0.2. Founders will ask investors one at a time until they get 5 yes. Let X=total # of investors asked. Find f(x) and f(10). <br>
  <script type="math/tex" id="MathJax-Element-87">X \not\sim NB</script> <br>
  Let Y = # who say no (Y+5=X) <br>
  <script type="math/tex" id="MathJax-Element-88">Y \sim NB(5, 0.2)</script> <br>
  So <br>
  <script type="math/tex; mode=display" id="MathJax-Element-89">\begin{equation}
\begin{split}
f(x) & = P(X=x) \\ &= P(Y= x-5) \\ & ={x-5+5-1 \choose 5-1} {0.2}^5 0.8^{x-5} \\
& = {x-1 \choose 4}0.2^50.8^{x-5} \ \text{for x=5,6,7,...} \\
f(10) &= {9 \choose 4}0.2^50.8^5 =0.013
\end{split}
\end{equation}</script></p>
</blockquote>



<h3 id="geometric-rv-56">Geometric rv (5.6)</h3>

<p>(nothing to do with hypergeometric) <br>
Special case of NB with k=1. <br>
X= # of F’s before obtaining the first S in Bernoulli trails <br>
X~Geo(p) <br>
range: {0, 1, 2, …}</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-90">\begin{equation}
\begin{split}
pf \ \ \ f(x) & = P(X=x) \\
& = P(x \ \ \text{F's, then 1 S}) \\
 f(x) & = (1-p)^xp 
\end{split}
\end{equation}</script> <br>
no orderings to worry about since it’s just FFFF….FS (x F’s) <br>
Easy to show <script type="math/tex" id="MathJax-Element-91">\Sigma_{x=0}^\infty f(x) =1</script> using infinite geometric series formula <br>
<script type="math/tex" id="MathJax-Element-92">F(x) = P(X \leq x) = 1 - P(X \leq x+1) \\= 1 - (p(1-p)^{x+1} + p(1-p)^{x+2} + p(1-p)^{x+3} + ...)</script></p>

<table>
<thead>
<tr>
  <th></th>
  <th>Discrete Uniform</th>
  <th>Hypergeometric</th>
  <th>Bin</th>
  <th>NB</th>
  <th>Geometric</th>
  <th>Poisson</th>
</tr>
</thead>
<tbody><tr>
  <td>pf f(x)</td>
  <td><script type="math/tex" id="MathJax-Element-93">\frac{1}{b-a+1}</script></td>
  <td><script type="math/tex" id="MathJax-Element-94">\frac{{r \choose x} {N-r \choose n-x}}{{N \choose n}}</script></td>
  <td><script type="math/tex" id="MathJax-Element-95">{n \choose x}p^x(1-p)^{n-x}</script></td>
  <td><script type="math/tex" id="MathJax-Element-96">{x + k -1 \choose k-1}p^k(1-p)^x</script></td>
  <td><script type="math/tex" id="MathJax-Element-97">(1-p)^xp</script></td>
  <td><script type="math/tex" id="MathJax-Element-98">\frac{e^{-\lambda t} (\lambda t) ^x}{x!}</script></td>
</tr>
<tr>
  <td>range</td>
  <td>{a, a+1, .. ,b}</td>
  <td>weird!</td>
  <td>0,…,n</td>
  <td>0,1,…</td>
  <td>0,1 …</td>
  <td>0,1,2,…</td>
</tr>
<tr>
  <td>cdf F(x)</td>
  <td><script type="math/tex" id="MathJax-Element-99">\frac{x-a+1}{b-a+1}</script></td>
  <td>no closed form</td>
  <td>no</td>
  <td>no</td>
  <td><script type="math/tex" id="MathJax-Element-100">1-(1-p)^{x+1}</script></td>
  <td><script type="math/tex" id="MathJax-Element-101">F(x) = e^{-\lambda t} (1 + \frac{\lambda t}{1!} +\frac{(\lambda t)^2}{2!} + ... + \frac{(\lambda t)^x}{x!})</script></td>
</tr>
<tr>
  <td>when to use?</td>
  <td><script type="math/tex" id="MathJax-Element-102">- \text{fixed # values} \\ - \text{equally likely}</script></td>
  <td><script type="math/tex" id="MathJax-Element-103">- \text{w/o replacement} \\ - \text{know # S in a subset}</script></td>
  <td><script type="math/tex" id="MathJax-Element-104">- \text{Bernoulli trials - indep, S or F, P(S)} \\</script> <script type="math/tex" id="MathJax-Element-105"> -\text{know # trials} \\</script> - counting # S</td>
  <td><script type="math/tex" id="MathJax-Element-106">- \text{Bernoulli trials - indep, S or F, P(S)} \\</script> <script type="math/tex" id="MathJax-Element-107">- \text{ know # S's $\\$} \\ -\text{"before" "until" "waiting for"}</script></td>
  <td><script type="math/tex" id="MathJax-Element-108">- \text{Bernoulli trials - indep, S or F, P(S)} \\</script> <script type="math/tex" id="MathJax-Element-109">- \text{same but k=1}</script></td>
  <td>Bin with large n, small p (approx)</td>
</tr>
</tbody></table>


<p>f(x) <br>
When to use? = Bin with large n, small p (approx) <br>
range 0,1,2,…</p>



<h2 id="lecture-15">Lecture - 15</h2>

<blockquote>
  <p>Recall: uniform, hypergeometric, binomial, NB, geometric <br>
  Poisson dist from Bin (5.7) <br>
  Poisson process (5.8)</p>
</blockquote>



<h3 id="poisson-rv-57">Poisson rv (5.7)</h3>

<p>We say X has a Poisson distribution with parameter <script type="math/tex" id="MathJax-Element-110">\mu</script> (<script type="math/tex" id="MathJax-Element-111">X\sim Poi(\mu))</script> if <script type="math/tex" id="MathJax-Element-112">f(x) = \frac{e^{-\mu}\mu^x}{x!}</script> for <script type="math/tex" id="MathJax-Element-113">x=0,1,2,...</script> <br>
Easy to show <script type="math/tex" id="MathJax-Element-114">\Sigma_{x=0}^{\infty}f(x)=1</script> since <script type="math/tex" id="MathJax-Element-115">e^\mu=\Sigma_{x=0}^{\infty} \frac{{\mu}^x}{x!}</script> <br>
The Poisson is a limiting case of the Binomial when n-&gt;<script type="math/tex" id="MathJax-Element-116">\infty</script> and p-&gt;0 such that the product np remains constant  <br>
<script type="math/tex" id="MathJax-Element-117">f(x) = {n \choose x}p^x (1-p)^{n-x}</script> <br>
Let np = <script type="math/tex" id="MathJax-Element-118">\mu</script> <br>
then,  <br>
<script type="math/tex; mode=display" id="MathJax-Element-119">\begin{equation}
\begin{split}
p &=\frac{\mu}{n} \\
 &= \frac{n(n-1)...(n-x+1)}{x!}\frac{\mu}{n}^x(1-\frac{\mu}{n})^{n-x} \\
&=\frac{\mu}{x!}^x\frac{n}{n}\frac{n-1}{n}...\frac{n-x+1}{n}(1-\frac{\mu}{n})^n(1-\frac{\mu}{n})^{-x} \\
\text{now let n} \rightarrow \infty \\
&=\frac{\mu^x}{x!}*1*1...1*e^{-\mu}*1 \\
&=\frac{e^{-\mu}\mu^x}{x!} \text{ which is } Poi(\mu=np)
\end{split}
\end{equation}</script> <br>
So if we have a Bin(n,p) with large n and small p, we can approx if with a Poisson rv with parameter <script type="math/tex" id="MathJax-Element-120">\mu = np</script>. Guideline: <script type="math/tex" id="MathJax-Element-121">n\geq40</script> and <script type="math/tex" id="MathJax-Element-122">p\leq 0.05</script> works well!</p>

<blockquote>
  <p>Example: <br>
  Roll up the Rim - “1 in 9 cups win!” You buy 100 cups (treat as independent). Find prob you get 10 or fewer winning cups.</p>
</blockquote>

<p>X= # winning <br>
<script type="math/tex" id="MathJax-Element-123">X \sim Bin(100,\frac{1}{9})</script> <br>
So: <br>
<script type="math/tex; mode=display" id="MathJax-Element-124">\begin{equation}
\begin{split}
P(X\leq 10) & = f(0) + f(1) + ... + f(10) \\
& = {100 \choose 0} \frac{1}{9}^0\frac{8}{9}^100 + {100 \choose 1} \frac{1}{9}^1\frac{8}{9}^99 + ... + \text{tedious calc} \\
&=0.439
\end{split}
\end{equation}</script></p>

<p>Try Poisson approx <script type="math/tex" id="MathJax-Element-125">\mu = 100*\frac{1}{9} = 11.111</script> <br>
<script type="math/tex" id="MathJax-Element-126">Y \sim Poi(11.111)</script> <br>
<script type="math/tex" id="MathJax-Element-127">P(Y\leq 10) = \frac{e^{-11.1}}{0!}11.1^0+\frac{e^{-11.1}}{1!}11.1^1 + ... + \frac{e^{-11.1}}{11!}11.1^{10} \\= e^{-11.1}(1 + 11.1 + \frac{11.1}{2!}^2 + ... + \frac{11.1}{10!}^10) = 0.447</script></p>

<p>Not a great approx since <script type="math/tex" id="MathJax-Element-128">p = \frac{1}{9}</script> was a bit too high to be “close to 0”</p>

<blockquote>
  <p>Clicker question <br>
  Suppose you type at exactly 90 words pm and on each word have a 1% chance of making an error. After 1 minute, what is the probability you have made NO errors? <br>
  0.405 -&gt; from bin. 0.407 -&gt; from poisson</p>
</blockquote>

<p>You can also use Poisson when <script type="math/tex" id="MathJax-Element-129">p\approx 1</script> by instead modelling the number of F’s instead of S’s.</p>



<h3 id="poisson-process-58">Poisson Process (5.8)</h3>

<p>Consider “events” occurring randomly throughout time/space according to 3 conditions:</p>

<ol>
<li>Independence  <br>
<ol><li>(events have no impact on each other)</li>
<li># of events in non-overlapping time intervals are indep.</li></ol></li>
<li>Individuality  <br>
<ol><li>(events occur one at a time)</li>
<li>Cannot have two or more events at the exact same time</li></ol></li>
<li>Homogeneity/Uniformity <br>
<ol><li>(events occur at a constant rate <script type="math/tex" id="MathJax-Element-130">\lambda</script>)</li>
<li>Prob of an event occurring in a short time interval (t, t+ <script type="math/tex" id="MathJax-Element-131">\Delta</script>t) is proportional to <script type="math/tex" id="MathJax-Element-132">\lambda \Delta t</script></li>
<li>Can’t have periods of higher activity</li></ol></li>
</ol>

<blockquote>
  <p>E.g. emails into an inbox <br>
  Cars through an intersection <br>
  Births in a large population</p>
</blockquote>



<h2 id="lecture-16">Lecture - 16</h2>

<p>Imagine we observe a Poisson process (with rate <script type="math/tex" id="MathJax-Element-133">\lambda</script>) for t units of time. <br>
Let X = # of events that occur. <br>
X is a discrete rv with no maximum. <br>
It turns out that: <br>
X ~ Poi(<script type="math/tex" id="MathJax-Element-134">\mu</script> = Xt) <br>
ie. <script type="math/tex" id="MathJax-Element-135">f(x) = \frac{e^{-\lambda t} (\lambda t) ^x}{x!}</script> for x = 0, 1, 2</p>

<blockquote>
  <p>Proof: See course notes <br>
  Add one more column to your chart!</p>
</blockquote>

<p>| Poisson <br>
f(x) | <script type="math/tex" id="MathJax-Element-136">\frac{e^{-\lambda t} (\lambda t) ^x}{x!}</script> | <script type="math/tex" id="MathJax-Element-137">F(x) = e^{-\lambda t} (1 + \frac{\lambda t}{1!} +\frac{(\lambda t)^2}{2!} + ... + \frac{(\lambda t)^x}{x!})</script> <br>
When to use? = Bin with large n, small p (approx) <br>
range 0,1,2,…</p>

<ul>
<li>Conditions of a Poisson process, counting # of events in a fixed time period</li>
</ul>

<p>When NOT to use? <br>
We can specify a maximum <br>
If it makes sense to ask how many time the event did not occur</p>

<p>Example:</p>

<blockquote>
  <p>requests to a web server follow the conditions of a Poisson process with rate 100 per minute. <br>
  Find prob of 1 request in 1 sec. 90 requests in 1 min.</p>
</blockquote>

<p>Let X = # requests in 1 sec <br>
X ~ Poi(<script type="math/tex" id="MathJax-Element-138">\mu = 100 * \frac{1}{60}</script> or <script type="math/tex" id="MathJax-Element-139">\frac{100}{60} = \frac{5}{3}</script>) <br>
<script type="math/tex" id="MathJax-Element-140">P(X=1)=\frac{e^{-\frac{5}{3}}(\frac{5}{3})}{1!} = 0.314</script> <br>
Let Y = # requrests in 1 min <br>
Y ~ Poi(<script type="math/tex" id="MathJax-Element-141">\mu = 100*1</script>) <br>
P(Y=90) = <script type="math/tex" id="MathJax-Element-142">\frac{e^{-100}100^{90}}{90!} = 0.025</script></p>



<h3 id="combining-models-59">Combining Models (5.9)</h3>

<p>Many problems may combine more than one distribution together. Your task is to identify the distribution needed, depending on the perobability requested.</p>

<blockquote>
  <p>Example: Server requests 100/min <br>
  A 1-second period is “quiet” if it contains no requests. </p>
</blockquote>

<p>a) Find the Prob of a “quiet” second.  <br>
X=# requests in 1 sec. X~Poi(<script type="math/tex" id="MathJax-Element-143">\frac{5}{3}</script>) <br>
P(X = 0) = <script type="math/tex" id="MathJax-Element-144">\frac{e^{-\frac{5}{3}}(\frac{5}{3})^0}{0!} = 0.189</script></p>

<p>b) Prob of 10 “quiet” seconds in a minute (60 non-overlapping sec) <br>
Let Y = # “quiet” in 60 sec. <br>
Y~Bin(60,0.189) <br>
P(Y=10) = <script type="math/tex" id="MathJax-Element-145">{60 \choose 10}0.189^{10}0.811^{50} = 0.124</script></p>

<p>c) Prob of having to wait 30 non-overlapping sec to get 2 “quiet” <br>
Let Z = # non-quiet sec before 2 “quiet” <br>
Z ~ NB(2, 0.189) <br>
P(Z = 28) = <script type="math/tex" id="MathJax-Element-146">{28 + 2 -1 \choose 28}0.189^2 0.811^{28} = 0.003</script></p>

<p>d) Given (c), the prob there is 1 “quiet” sec in the first 15 sec. <br>
P(1 Q in 15 sec | wait 30 for 2 Q) <br>
<script type="math/tex" id="MathJax-Element-147">= \frac{P(\text{1 Q in 15 sec AND wait 30 for 2Q})}{P(\text{wait 30 for 2 Q})}</script> <br>
<script type="math/tex" id="MathJax-Element-148">= \frac{P(\text{1 Q in 15 sec})P(\text{wait 15 more to get one more Q)}}{P(\text{wait 30 for 2 Q})}</script> <br>
Use binomial for 1st prob. Geometric for 2nd. <br>
<script type="math/tex" id="MathJax-Element-149">= \frac{({15 \choose 1}0.189^10.811^{14})(0.811^{14}0.189)}{{29 \choose 28}0.189^{2}0.811^{28}}</script></p>



<h2 id="lecture-17">Lecture 17</h2>



<h3 id="ch-7-expected-value-variance-summarizing-data">Ch 7. Expected Value + Variance Summarizing Data</h3>

<p>Let X = # of kids in a family</p>

<p>To summarize the data, we can use: <br>
1. A frequency distribution</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>frequency</th>
</tr>
</thead>
<tbody><tr>
  <td>1</td>
  <td>10</td>
</tr>
<tr>
  <td>2</td>
  <td>14</td>
</tr>
<tr>
  <td>3</td>
  <td>10</td>
</tr>
<tr>
  <td>4</td>
  <td>1</td>
</tr>
<tr>
  <td>5</td>
  <td>2</td>
</tr>
</tbody></table>


<p>2. a frequency histogram (different graph here because I was lazy and didn’t want to draw one in ms paint) <br>
<img src="https://www.mathsisfun.com/data/images/bar-graph-fruit.gif"> <br>
3. A single number representing the average or sample mean <br>
<script type="math/tex; mode=display" id="MathJax-Element-150">\begin{equation}
\begin{split}
\bar x &= \frac{\text{total # kids}}{\text{# families}}\\
& = \frac{1*10 + 2*14 + 3*10 + 4*1 + 5*2}{37}\\
& = \frac{82}{37}  \\
& = 2.216
\end{split}
\end{equation}</script> <br>
4. median - the middle value: 2 in this case <br>
5. mode - most common/frequent value (in this case, 2)</p>

<p>Expectation of a r.v. (7.2) <br>
We had the sample mean of the kids be <script type="math/tex" id="MathJax-Element-151">\bar x</script> <br>
<script type="math/tex" id="MathJax-Element-152">\Sigma_{x=1}^{5} (\text{relative frequency of answering x})</script></p>

<p>We can replace the observed relative frequency with a theoretical probability of the r.v. equally x -&gt; theoretical mean</p>

<p>2011 census: </p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>1</th>
  <th>2</th>
  <th>3</th>
  <th>4</th>
  <th>5</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>0.43</td>
  <td>0.4</td>
  <td>0.12</td>
  <td>0.04</td>
  <td>0.01</td>
</tr>
</tbody></table>


<p>so the theoretical mean of X is: <br>
<script type="math/tex" id="MathJax-Element-153">\Sigma_{x=1}^{5}xf(x) = 1* 0.43 + 2*0.4 + 3*0.12 + 4*0.04 + 5*0.01 = 1.8</script> <br>
and median is 2 since F(2) &gt; 0.5 and F(1) &lt; 0.5 <br>
and mode is 1</p>

<p>Def: The expected value or expectation or mean of a discrete r.v X is: <br>
<script type="math/tex" id="MathJax-Element-154">E[X] = \mu = \Sigma_{\text{all x } \epsilon \text{ range of X}}xf(x)</script></p>



<h2 id="lecture-18">Lecture 18</h2>

<blockquote>
  <p>Recall - Expected value (aka mean) of X is <script type="math/tex" id="MathJax-Element-155">E[X]=\mu=\Sigma_{\text{all  }\epsilon \text{ range of X}}xf(x)</script></p>
</blockquote>

<p>Can think of it as a weighted average of the values X can take with weights = probabilities or balance point of the histogram of f(x). Often we may be interested in the average value of the function of x. Eg. x = usage on phone. g(X) = cost of that usage.</p>

<p>Def: the expected value of g(X) for a discrete r.v. X is <script type="math/tex" id="MathJax-Element-156">E[g(X)] = \Sigma_{\text{all  }\epsilon \text{ range of X}}g(x)f(x)</script>  <br>
weighted average of the g(x) values that can occur. (e.g. g(X) = 1000 + 250x)</p>

<p>What if <script type="math/tex" id="MathJax-Element-157">g(X) = \frac{2000}{X}</script> ? <br>
Here, <script type="math/tex" id="MathJax-Element-158">E[g(X)] \neq g(E[x])</script> because g(x) is <strong>non-linear</strong> function of X. Expectation is a linear operator.</p>

<p>SO ONLY USE IF g(X) IS LINEAR <br>
Also: <br>
<script type="math/tex" id="MathJax-Element-159">E[aX + b] = aE[X] + b</script></p>



<h3 id="applications-of-expectation-53">Applications of Expectation (5.3)</h3>

<p>If we have the distribution of X ans we let Y = g(X), we can find E[Y] by either: <script type="math/tex" id="MathJax-Element-160">E[g(X)] = \Sigma_{\text{all x}}g(x)f(x)</script> or by finding the range and pf of Y and using <script type="math/tex" id="MathJax-Element-161">\Sigma_{\text{all y}}yf(y)</script></p>

<blockquote>
  <p>Example <br>
  Suppose the time to finish a part on a coding question is 10 minutes if you make no errors. Syntax errors take 2 mins to fix. Logic errors take 10 min. Assume O(syntax error) = 0.1 , P(logic error) = 0.2 independently.</p>
</blockquote>

<p>Find the average(expected) time to finish this question. <br>
Let X = 0 (no errors), 1(syntax), 2(logic), 3(both),</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>0</th>
  <th>1</th>
  <th>2</th>
  <th>3</th>
</tr>
</thead>
<tbody><tr>
  <td>f(y)  -&gt; f(x)</td>
  <td>0.72 =(0.9*0.8)</td>
  <td>(0.1)*(0.8) = .08</td>
  <td>(0.9)*(0.2)=0.18</td>
  <td>0.02</td>
</tr>
<tr>
  <td>y -&gt; g(x)</td>
  <td>10</td>
  <td>12</td>
  <td>20</td>
  <td>22</td>
</tr>
</tbody></table>


<p>So <script type="math/tex" id="MathJax-Element-162">E[g(X)] = \Sigma_{x=0}^{3}g(x)f(x) \\ = 10*0.72 + 12*0.08 + 20*0.18 + 22*0.02 = 12.2</script></p>

<p>Example: <br>
A web server has a cache. 20% chance that the request is found in the cache (cache hit) -&gt; 10 ms. If it’s not found (cache miss) then it takes 50(send msg) + 70(lookup) + 50(return answer) ms. Find the expected time with and without the cache. <br>
Without: time is always 190ms So E[T] = 170. <br>
With: Let X = 0 if found, 1 if not found</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>0</th>
  <th>1</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>0.2</td>
  <td>0.8</td>
</tr>
<tr>
  <td>time=g(x)</td>
  <td>10</td>
  <td>180</td>
</tr>
</tbody></table>




<h2 id="lecture-19">Lecture 19</h2>

<p>Recall: <br>
<script type="math/tex; mode=display" id="MathJax-Element-163">\begin{equation}
\begin{split}
E[g(X)] &= \Sigma_{\text{all x}}g(x)f(x)\\
E[aX + b] &= aE[X]+b \text{(linear operator)}
\end{split}
\end{equation}</script></p>



<h3 id="means-and-variances-of-named-distributions-74">Means (and variances) of named distributions (7.4)</h3>

<ol>
<li>Binomial <br>
<ul><li>Let X~ Bin(n,i) <br>
Find E[X]: <br>
<script type="math/tex" id="MathJax-Element-164">E[X] = \Sigma_{allx}xf(x) \\ = \Sigma_{x=0}^{n}x{n \choose x}p^x(1-p)^{n-x} \\ = \Sigma_{x=1}^{n}x{n \choose x}p^x(1-p)^{n-x}</script> <br>
since the x=0 term is 0 <br>
<script type="math/tex" id="MathJax-Element-165">=\Sigma_{x=1}^{n}x\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} </script> <br>
Note: If <script type="math/tex" id="MathJax-Element-166">x \leq 1</script>, then <script type="math/tex" id="MathJax-Element-167">x! = x(x-1)!</script> <br>
<script type="math/tex" id="MathJax-Element-168">= \Sigma_{x=1}^{n}\frac{n!}{(x-1)!(n-x)!}p^x(1-p)^{n-x}</script> <br>
<script type="math/tex" id="MathJax-Element-169">= \Sigma_{x=1}^{n}\frac{n(n-1)!}{(x-1)!((n-1)-(x-1))!}pp^{x-1}(1-p)^{(x-1)-(x-1)}</script> <br>
<script type="math/tex" id="MathJax-Element-170">= np\Sigma_{x=1}^{n}{n-1 \choose x-1}p^{x-1}(1-p)^{(n-1)-(x-1)}</script> <br>
Let <script type="math/tex" id="MathJax-Element-171">y = x-1</script> <br>
<script type="math/tex" id="MathJax-Element-172">np\Sigma_{y=0}^{n-1}{n-1 \choose y}p^y(1-p)^{n-1-y}</script> <br>
The summation is just f(y) for Y~Bin(n-1,y) and that = 1 <br>
<script type="math/tex" id="MathJax-Element-173">\therefore E[X] = np</script> for X~Bin(n,p) <br>
This makes logical sense because the number of successes is proportional to both the # of trials and the prob of success. So the average # S is the # trials x prob of S.</li></ul></li>
<li>Poisson <br>
<ul><li>Let X~Poi(<script type="math/tex" id="MathJax-Element-174">\mu</script>) (where <script type="math/tex" id="MathJax-Element-175">\mu</script> comes from <script type="math/tex" id="MathJax-Element-176">\lambda t</script> or np or given) <br>
Find E[X] <br>
<script type="math/tex" id="MathJax-Element-177">E[X] = \Sigma_{x=0}^{\infty}x\frac{e^{-\mu}\mu^x}{x!}</script> <br>
again the x=0 term is 0 and for <script type="math/tex" id="MathJax-Element-178">x \leq 1</script>, x! =x(x-1)! <br>
<script type="math/tex" id="MathJax-Element-179">= \Sigma_{x=1}^{\infty}\frac{e^{-\mu}\mu^x}{(x-1)!}</script> <br>
<script type="math/tex" id="MathJax-Element-180">= \mu\Sigma_{x=1}^{\infty}\frac{e^{-\mu}\mu^{x-1}}{(x-1)!}</script> <br>
Let y = x-1 <br>
<script type="math/tex" id="MathJax-Element-181">= \mu \Sigma_{y=0}^{\infty}\frac{e^{-\mu}\mu^y}{y!}</script> <br>
that sum is just f(y) for Y~Poi(<script type="math/tex" id="MathJax-Element-182">\mu</script>) so <script type="math/tex" id="MathJax-Element-183">\Sigma_{y=1}^{\infty}f(y)=1</script> <br>
<script type="math/tex" id="MathJax-Element-184">\therefore E[X] = \mu</script> for X~Poi(<script type="math/tex" id="MathJax-Element-185">\mu</script>) <br>
i.e. <script type="math/tex" id="MathJax-Element-186">\mu</script> is the parameter and the mean. <br>
This makes sense if <script type="math/tex" id="MathJax-Element-187">\mu = \lambda t</script> since avg # events in t units of time should be the rate (<script type="math/tex" id="MathJax-Element-188">\frac{\text{avg # events}}{\text{time unit}}</script>) multiplied by the length of the time interval we observe.</li></ul></li>
<li>Similarly: <br>
<ol><li>X~NB(k,p) , <script type="math/tex" id="MathJax-Element-189">E[X] = \frac{k(1-p)}{p}</script></li>
<li>X~Hyp(N,r,n) , <script type="math/tex" id="MathJax-Element-190">E[X] = \frac{nr}{N}</script></li>
<li>X~DU[a,b] , <script type="math/tex" id="MathJax-Element-191">E[X] = \frac{a+b}{2}</script> <br>
these results can be proven from first principles (<script type="math/tex" id="MathJax-Element-192">\Sigma_{\text{all x}}xf(x)</script>) but there are other easier ways to show them</li></ol></li>
</ol>

<p>The mean of X, E[X] tells us where the distribution is centered, on average. But the practice we also care about how widely spread out the distribution is around that mean. <br>
E.g. determining the number of servers for an online system. need to know spread. <br>
How to measure? <br>
<script type="math/tex" id="MathJax-Element-193">E[X-\mu] = 0 </script> (not helpful!) <br>
<script type="math/tex" id="MathJax-Element-194">E[|X-\mu|]</script> - average absolute distance from mean</p>

<ul>
<li>need cases to evaluate</li>
<li>point of non-differentiabililty at <script type="math/tex" id="MathJax-Element-195">\mu</script></li>
</ul>

<p>So we use: <br>
<script type="math/tex" id="MathJax-Element-196">E[(X-\mu)^2]</script> - average squared distance from mean</p>



<h2 id="lecture-20">Lecture 20</h2>

<blockquote>
  <p>Def: Variance of a r.v. X is <script type="math/tex" id="MathJax-Element-197">Var(X)= \sigma^2 = E[(X-\mu)^2]</script> <br>
  Weighted average squared, distance from mean. If X is discrete, <script type="math/tex" id="MathJax-Element-198">Var(X) = \Sigma_{\text{all x}}(x-\mu)^2f(x)</script></p>
</blockquote>

<p>Calc form of variance: <br>
<script type="math/tex; mode=display" id="MathJax-Element-199">\begin{equation}
\begin{split}
Var(X) &= E[(X-\mu)^2] \\
&= E[X^2 - 2\mu X + \mu^2] \\
&= E[X^2] - 2\mu E[X] + \mu^2 \text{ by linearity} \\
&= E[X^2] - 2\mu^2 + \mu^2 \text{ since }E[X] = \mu \\
&= E[X^2] - \mu^2 \\
&= E[X^2] - E[X]^2
\end{split}
\end{equation}</script> <br>
Where <script type="math/tex" id="MathJax-Element-200">E[X^2] = \Sigma_{\text{all x}}xf(x)</script></p>

<p>Example: <br>
X has pf</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>10</th>
  <th>12</th>
  <th>20</th>
  <th>22</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>0.72</td>
  <td>0.08</td>
  <td>0.18</td>
  <td>0.62</td>
</tr>
</tbody></table>


<p>We found <script type="math/tex" id="MathJax-Element-201">E[X] = 12.2</script> min <br>
Now <script type="math/tex" id="MathJax-Element-202">E[X^2] = 10^2 *0.72 + 12^2*0.08 + 20^2*0.18 + 22^2*0.02 = 165.2</script> <br>
<script type="math/tex" id="MathJax-Element-203">\therefore Var(X) = 165.2 - 12.2^2 = 16.36 \text{ min}^2</script></p>

<blockquote>
  <p>Note that variance is measured in units squared rather than the original units of X. Taking the square root of the variance makes more sense.</p>
</blockquote>

<p><strong>Defn</strong>: the standard deviation of X is <script type="math/tex" id="MathJax-Element-204">SD(X) = \sigma = \sqrt{Var(X)}</script></p>

<p>In our example, <script type="math/tex" id="MathJax-Element-205">SD(X) = \sqrt{16.36} = 4.04</script> min <br>
Remember, Var(X) will always be non-negative because it’s a weighted average of <script type="math/tex" id="MathJax-Element-206">(x-\mu)^2</script> values <script type="math/tex" id="MathJax-Element-207">\geq 0</script> <br>
So SD(X) will also be <script type="math/tex" id="MathJax-Element-208">\geq 0</script> and a Real number</p>



<h3 id="mean-var-of-a-linear-fn-of-x">Mean + Var of a linear f’n of X</h3>

<p><script type="math/tex" id="MathJax-Element-209">Y=aX+b</script> <br>
<script type="math/tex" id="MathJax-Element-210">E[Y] = aE[X] + b</script> by linearity of expectations <br>
<script type="math/tex; mode=display" id="MathJax-Element-211">\begin{equation}
\begin{split}
Var(Y) &= E[(Y - E[Y])^2] \\
& = E[(aX + b - (aE[X] + b))^2] \\
&= E[a^2(X-E[X])^2] \\
&= a^2Var(X)
\end{split}
\end{equation}</script></p>

<blockquote>
  <p>Note: b does not affect the variance since shifting the distribution doesn’t affect the spread. We increase all the distances by a factor of <script type="math/tex" id="MathJax-Element-212">a</script> so the squared distances increase by <script type="math/tex" id="MathJax-Element-213">a^2</script></p>
</blockquote>

<p>Finally, <br>
<script type="math/tex" id="MathJax-Element-214">SD(Y) = \sqrt{Var{Y}} \\ = \sqrt{a^2Var(X)} \\ =|a|SD(x)</script></p>

<p>Example <br>
Suppose X has pf</p>

<table>
<thead>
<tr>
  <th>x</th>
  <th>0</th>
  <th>1</th>
  <th>2</th>
  <th>3</th>
  <th>4</th>
</tr>
</thead>
<tbody><tr>
  <td>f(x)</td>
  <td>0.1</td>
  <td>0.1</td>
  <td>0.1</td>
  <td>0.5</td>
  <td>0.2</td>
</tr>
<tr>
  <td>y</td>
  <td>1</td>
  <td>3</td>
  <td>5</td>
  <td>7</td>
  <td>9</td>
</tr>
</tbody></table>


<p>Let <script type="math/tex" id="MathJax-Element-215">Y = 2X + 1</script> <br>
<script type="math/tex" id="MathJax-Element-216">E[X] = 2.6</script> <br>
<script type="math/tex" id="MathJax-Element-217">E[X^2] = 8.2</script>  <br>
<script type="math/tex" id="MathJax-Element-218">E[Y] = 6.2</script>  <br>
<script type="math/tex" id="MathJax-Element-219">E[Y^2] = 44.2</script></p>

<p><script type="math/tex" id="MathJax-Element-220">Var(X) = E[X^2] - E[X] = 1.44</script> <br>
<script type="math/tex" id="MathJax-Element-221">Var(Y) = E[Y^2] - E[Y] = 5.76</script></p>

<p>But! We can calculate the Var(Y) in a different way! <br>
Verify <script type="math/tex" id="MathJax-Element-222">E[Y] = 2E[X]+1</script> and <script type="math/tex" id="MathJax-Element-223">Var(Y) = 2^2Var(X)</script></p>



<h3 id="variances-of-named-distributions">Variances of named distributions</h3>

<p></p><ol> <br>
<li>Poissson - X~Poi(<script type="math/tex" id="MathJax-Element-224">\mu</script>) , <script type="math/tex" id="MathJax-Element-225">E[X] = \mu</script> <br>
<ul><li><script type="math/tex" id="MathJax-Element-226">Var(X) = E[X^2] - E[X]</script> <br>
<script type="math/tex" id="MathJax-Element-227">= E[X(X-1) + X] - E[X]^2</script> <br>
<script type="math/tex" id="MathJax-Element-228">= E[X(X-1)] +E[X] - E[X]^2</script></li>
<li>Why? </li>
<li><script type="math/tex" id="MathJax-Element-229">E[X(X-1)] = \Sigma_{x=0}^{\infty}x(x-1)\frac{e^{-\mu}\mu^x}{x(x-1)(x-2)!} \\=\mu^2\Sigma_{y=0}^{\infty}\frac{e^{-\mu}\mu^y}{y!} = ty = x-2</script></li>
<li>So <script type="math/tex" id="MathJax-Element-230">Var(X) = \mu^2 + \mu - \mu^2 = \mu</script> <br>
In the Poisson, <script type="math/tex" id="MathJax-Element-231">\mu</script> is the mean and the variance!  <br>
<strong>^ the Poisson variance is a good exam question</strong></li></ul></li>
<li>Binomial - <script type="math/tex" id="MathJax-Element-232">E[X] = np</script> ,  <script type="math/tex" id="MathJax-Element-233">E[X(X-1)] = n(n-1)p^2</script> <br>
<ul><li><script type="math/tex" id="MathJax-Element-234">\therefore Var(X) = n(n-1)p^2 + np - (np)^2 = np(1-p)</script> <br></li></ul></li></ol><p></p>

<blockquote>
  <p>Note: Var is smaller than mean when <script type="math/tex" id="MathJax-Element-235">p \approx 0</script> or <script type="math/tex" id="MathJax-Element-236">p \approx 1</script> the variance is small since there’s less uncertainty
  </p>
</blockquote></body>
</html>
-->